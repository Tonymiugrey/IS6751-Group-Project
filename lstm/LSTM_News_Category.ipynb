{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText News Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Croya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Croya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Croya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Croya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords # remove stopword\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Vectorization classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
    "\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
    "        \"\"\"\n",
    "\n",
    "        # news_vocab._token_to_idx: {'<MASK>': 0, '<UNK>': 1, '<BEGIN>': 2, '<END>': 3, 'jobs': 4, 'tax': 5, 'cuts': 6,  \n",
    "        #                             ......, 'shiite': 3407, 'ghraib': 3408}\n",
    "        # category_vocab._token_to_idx: {'Business': 0, 'Sci/Tech': 1, 'Sports': 2, 'World': 3}\n",
    "        \n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "            \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        \"\"\"\n",
    "        return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "        \n",
    "        Args: \n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "\n",
    "        self._mask_token = mask_token             # for paddding, e.g., Wall St. Bears Claw Back Into the Black (Reuters)\n",
    "                                                  #               -> [2, 5, 6, 10, 10, 8, 7, 9, 19, ......., 3, 0, 0, 0, ..., 0]\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)            # mask_index is 0\n",
    "        self.unk_index = self.add_token(self._unk_token)              # unk_index is 1\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)  # begin_seq_index is 2\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)      # end_seq_index is 3\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsVectorizer(object):\n",
    "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"    \n",
    "    def __init__(self, news_vocab, category_vocab):\n",
    "        self.news_vocab = news_vocab\n",
    "        self.category_vocab = category_vocab\n",
    "\n",
    "    def vectorize(self, text, vector_length=-1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            text (str): the string of words separated by a space\n",
    "            vector_length (int): an argument for forcing the length of index vector\n",
    "        Returns:\n",
    "            the vetorized text (numpy.array)\n",
    "        \"\"\"\n",
    "        \"\"\"    \n",
    "        mask_index is 0\n",
    "        unk_index is 1\n",
    "        begin_seq_index is 2\n",
    "        end_seq_index is 3\n",
    "        \n",
    "        When text is \"Wall St. Bears Claw Back Into the Black (Reuters)\"; max vector length is 29 in current dataset \n",
    "        \n",
    "        out_vector = [2, 5, 6, 10, 10, 8, 7, 9, 19, ......., 3, 0, 0, 0, ..., 0]\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        indices = [self.news_vocab.begin_seq_index]\n",
    "        indices.extend(self.news_vocab.lookup_token(token) \n",
    "                       for token in text.split(\" \"))\n",
    "        indices.append(self.news_vocab.end_seq_index)\n",
    "\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "\n",
    "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        out_vector[:len(indices)] = indices\n",
    "        out_vector[len(indices):] = self.news_vocab.mask_index\n",
    "\n",
    "        return out_vector\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, news_df, cutoff=25):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        \n",
    "        Args:\n",
    "            news_df (pandas.DataFrame): the target dataset\n",
    "            cutoff (int): frequency threshold for including in Vocabulary \n",
    "        Returns:\n",
    "            an instance of the NewsVectorizer\n",
    "        \"\"\"\n",
    "        category_vocab = Vocabulary()        \n",
    "        for category in sorted(set(news_df.category)):\n",
    "            category_vocab.add_token(category)\n",
    "\n",
    "        word_counts = Counter()\n",
    "        for text in news_df.text:\n",
    "            for token in text.split(\" \"):\n",
    "                if token not in string.punctuation:\n",
    "                    word_counts[token] += 1\n",
    "        \n",
    "        news_vocab = SequenceVocabulary()\n",
    "        for word, word_count in word_counts.items():\n",
    "            if word_count >= cutoff:\n",
    "                news_vocab.add_token(word)\n",
    "        \n",
    "        return cls(news_vocab, category_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'glaxo settle paxil settle paxil suicide paxil suicide pill suicide pill suit pill suit new suit new york new york reuters york reuters glaxosmithkline reuters glaxosmithkline plc glaxosmithkline plc href plc href target href target stock target stock quickinfo stock quickinfo fullquote quickinfo fullquote l fullquote l agreed l agreed release agreed release clinical release clinical study clinical study drug study drug settle drug settle lawsuit settle lawsuit accused lawsuit accused withholding accused withholding negative withholding negative information negative information antidepressant information antidepressant paxil antidepressant paxil new paxil new york new york attorney york attorney general attorney general office general office said office said thursday'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_preprocessing(text):\n",
    "    text = text.lower() # case folding\n",
    "    text = re.sub('&\\w*\\;\\w*', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub(r\"[^a-z]+\", r\" \", text) # Regulation, remove special charecters\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    # remove stopwords and lemmatization\n",
    "    result = [lemmatizer.lemmatize(i) for i in tokens if not i in stop_words]\n",
    "    # result = result[:1000]\n",
    "    # bigram\n",
    "    bigram_result = []\n",
    "    bigram_list = ngrams(result, 3)\n",
    "    for word_set in bigram_list:\n",
    "        for word in word_set:\n",
    "            bigram_result.append(word)\n",
    "    return bigram_result\n",
    "\n",
    "test_str_1 = \"This is sentence to test the effect of preprocessing... Yeah~\\nCool!fac  feae ge fe ga ðŸª£ðŸ›€ðŸŽ€ â˜â˜¢ï¸Žâ¥â˜âŽ (>â•¹Ï‰â•¹<)å–µ\"\n",
    "test_str_2 = \"Glaxo Settles Paxil 'Suicide Pill' Suit.  NEW YORK (Reuters) - GlaxoSmithKline Plc &lt;A HREF=http://www.investor.reuters.com/FullQuote.aspx?ticker=GSK.L target=/stocks/quickinfo/fullquote\"\"&gt;GSK.L&lt;/A&gt; has agreed  to release all clinical studies of its drugs to settle a  lawsuit that accused it of withholding negative information  about the antidepressant Paxil, the New York Attorney General's  office said on Thursday.\"\n",
    "list = text_preprocessing(test_str_2)\n",
    "' '.join(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, news_df, vectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            news_df (pandas.DataFrame): the dataset\n",
    "            vectorizer (NewsVectorizer): vectorizer instatiated from dataset\n",
    "        \"\"\"\n",
    "        self.news_df = news_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        # +1 if only using begin_seq, +2 if using both begin and end seq tokens\n",
    "        measure_len = lambda context: len(context.split(\" \"))\n",
    "        self._max_seq_length = max(map(measure_len, news_df.text)) + 2\n",
    "        \n",
    "\n",
    "        self.train_df = self.news_df[self.news_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.news_df[self.news_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.news_df[self.news_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "        # Class weights\n",
    "        class_counts = news_df.category.value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.category_vocab.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "        \n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, news_csv):\n",
    "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
    "        \n",
    "        Args:\n",
    "            surname_csv (str): location of the dataset\n",
    "        Returns:\n",
    "            an instance of SurnameDataset\n",
    "        \"\"\"\n",
    "        news_df = pd.read_csv(news_csv)\n",
    "        \n",
    "        for index, text in enumerate(news_df.text):\n",
    "            processed_list = text_preprocessing(text)\n",
    "            news_df.text[index] = ' '.join(processed_list)\n",
    "\n",
    "        train_news_df = news_df[news_df.split=='train']\n",
    "        return cls(news_df, NewsVectorizer.from_dataframe(train_news_df))\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" selects the splits in the dataset using a column in the dataframe \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        \n",
    "        Args:\n",
    "            index (int): the index to the data point \n",
    "        Returns:\n",
    "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        news_vector = \\\n",
    "            self._vectorizer.vectorize(row.text, self._max_seq_length)\n",
    "\n",
    "        category_index = \\\n",
    "            self._vectorizer.category_vocab.lookup_token(row.category)\n",
    "\n",
    "        return {'x_data': news_vector,     # e.g., \"Wall St. Bears Claw Back Into the Black (Reuters)\" \n",
    "                                            # -> [2, 5, 6, 10, 10, 8, 7, 9, 19, ......., 3, 0, 0, 0, ..., 0]\n",
    "                'y_target': category_index} # e.g., 2\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"): \n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model: NewsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsClassifier(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, output_dim, hidden_dim, num_layers, bidirectional, pretrained_embeddings=None):\n",
    "        super(NewsClassifier, self).__init__()\n",
    "\n",
    "        if pretrained_embeddings is None:\n",
    "            self.embedding = nn.Embedding(num_embeddings, embedding_dim) \n",
    "            self.embedding.weight.requires_grad = True\n",
    "        else:\n",
    "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False) \n",
    "                                                                          \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=bidirectional, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        else:\n",
    "            hidden = hidden[-1, :, :]\n",
    "        out = self.fc(hidden)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Routine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"Handle the training state updates.\n",
    "\n",
    "    Components:\n",
    "     - Early Stopping: Prevent overfitting.\n",
    "     - Model Checkpoint: Model is saved if the model is better\n",
    "\n",
    "    :param args: main arguments\n",
    "    :param model: model to train\n",
    "    :param train_state: a dictionary representing the training state values\n",
    "    :returns:\n",
    "        a new train_state\n",
    "    \"\"\"\n",
    "\n",
    "    # Save one model at least\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # If loss worsened\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # Loss decreased\n",
    "        else:\n",
    "            # Save the best model\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "                train_state['early_stopping_best_val'] = loss_t  # update 'early_stopping_best_val'\n",
    "\n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### general utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda, mps):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        print(\"cuda manual seed\", seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    try:\n",
    "        if mps:\n",
    "            print(\"mps manual seed\", seed)\n",
    "            torch.mps.manual_seed(seed)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "        \n",
    "def load_glove_from_file(glove_filepath):\n",
    "    \"\"\"\n",
    "    Load the GloVe embeddings \n",
    "    \n",
    "    Args:\n",
    "        glove_filepath (str): path to the glove embeddings file \n",
    "    Returns:\n",
    "        word_to_index (dict), embeddings (numpy.ndarary)\n",
    "    \"\"\"\n",
    "\n",
    "    word_to_index = {}\n",
    "    embeddings = []\n",
    "    with open(glove_filepath, \"r\", encoding='utf8') as fp:\n",
    "        for index, line in enumerate(fp):\n",
    "            line = line.split(\" \") # each line: word num1 num2 ...\n",
    "            word_to_index[line[0]] = index # word = line[0] \n",
    "            embedding_i = np.array([float(val) for val in line[1:]])\n",
    "            embeddings.append(embedding_i)\n",
    "    return word_to_index, np.stack(embeddings)\n",
    "\n",
    "def make_embedding_matrix(glove_filepath, words):\n",
    "    \"\"\"\n",
    "    Create embedding matrix for a specific set of words.\n",
    "    \n",
    "    Args:\n",
    "        glove_filepath (str): file path to the glove embeddigns\n",
    "        words (list): list of words in the dataset\n",
    "    \"\"\"\n",
    "    word_to_idx, glove_embeddings = load_glove_from_file(glove_filepath)\n",
    "    embedding_size = glove_embeddings.shape[1]\n",
    "    \n",
    "    final_embeddings = np.zeros((len(words), embedding_size))\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if word in word_to_idx:\n",
    "            final_embeddings[i, :] = glove_embeddings[word_to_idx[word]]\n",
    "        else:\n",
    "            embedding_i = torch.ones(1, embedding_size)\n",
    "            torch.nn.init.xavier_uniform_(embedding_i)\n",
    "            final_embeddings[i, :] = embedding_i\n",
    "\n",
    "    return final_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings and some prep work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\t../model_storage/News_Category\\model_LSTM_News_Category.pth\n",
      "Using CUDA: True\n",
      "Using MPS: False\n",
      "cuda manual seed 1337\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Data and Path hyper parameters\n",
    "    news_csv=\"../data/processed/News_Category_Dataset_with_splits.csv\",\n",
    "    model_state_file=\"model_LSTM_News_Category.pth\",\n",
    "    save_dir=\"../model_storage/News_Category\",\n",
    "    # Model hyper parameters\n",
    "    glove_filepath='../data/glove/glove.6B.100d.txt', \n",
    "    use_glove=True,\n",
    "    embedding_dim=100, \n",
    "    hidden_dim=128,\n",
    "    output_dim=4,\n",
    "    num_layers=2,\n",
    "    bidirectional=True,\n",
    "    # Training hyper parameter\n",
    "    seed=1337, \n",
    "    learning_rate=0.001, \n",
    "    batch_size=64, \n",
    "    num_epochs=100, \n",
    "    early_stopping_criteria=5, \n",
    "    # Runtime option\n",
    "    cuda=True,\n",
    "    mps=True, \n",
    "    catch_keyboard_interrupt=True, \n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True\n",
    ") \n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)    \n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "    \n",
    "# Check CUDA for Nvidia\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "# Check MPS for Mac\n",
    "if not torch.backends.mps.is_available():\n",
    "    args.mps = False\n",
    "    \n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"mps\" if args.mps else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "print(\"Using MPS: {}\".format(args.mps))\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed_everywhere(args.seed, args.cuda, args.mps)\n",
    "\n",
    "# handle dirs\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pre-trained embeddings\n"
     ]
    }
   ],
   "source": [
    "# create dataset and vectorizer\n",
    "dataset = NewsDataset.load_dataset_and_make_vectorizer(args.news_csv)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "# Use GloVe or randomly initialized embeddings\n",
    "if args.use_glove:\n",
    "    words = vectorizer.news_vocab._token_to_idx.keys()  # 3409 unique words\n",
    "    embeddings = make_embedding_matrix(glove_filepath=args.glove_filepath,     # embeddings: (3409, 100)\n",
    "                                       words=words)\n",
    "    print(\"Using pre-trained embeddings\")\n",
    "else:\n",
    "    print(\"Not using pre-trained embeddings\")\n",
    "    embeddings = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = NewsClassifier(embedding_dim=args.embedding_dim,          # e.g, 100\n",
    "                            num_embeddings=len(vectorizer.news_vocab),  # e.g., 3409\n",
    "                            output_dim=args.output_dim,\n",
    "                            hidden_dim=args.hidden_dim,\n",
    "                            num_layers=args.num_layers,\n",
    "                            bidirectional=args.bidirectional,\n",
    "                            pretrained_embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<MASK>': 0, '<UNK>': 1, '<BEGIN>': 2, '<END>': 3, 'guidant': 4, 'johnson': 5, 'indianapolis': 6, 'dec': 7, 'announcement': 8, 'thursday': 9, 'morning': 10, 'end': 11, 'week': 12, 'speculation': 13, 'mean': 14, 'shareholder': 15, 'sign': 16, 'indiana': 17, 'lose': 18, 'one': 19, 'fortune': 20, 'company': 21, 'british': 22, 'energy': 23, 'seek': 24, 'approval': 25, 'stock': 26, 'market': 27, 'fight': 28, 'plan': 29, 'rebel': 30, 'oppose': 31, 'restructuring': 32, 'buy': 33, 'back': 34, 'bln': 35, 'raise': 36, 'dividend': 37, 'update': 38, 'general': 39, 'electric': 40, 'co': 41, 'biggest': 42, 'value': 43, 'said': 44, 'much': 45, 'billion': 46, 'three': 47, 'year': 48, 'raised': 49, 'quarterly': 50, 'percent': 51, 'analyst': 52, 'estimated': 53, 'bhp': 54, 'confidence': 55, 'china': 56, 'played': 57, 'chinese': 58, 'economy': 59, 'friday': 60, 'demand': 61, 'would': 62, 'quot': 63, 'long': 64, 'term': 65, 'asian': 66, 'lower': 67, 'greenspan': 68, 'awaited': 69, 'singapore': 70, 'reuters': 71, 'edged': 72, 'wednesday': 73, 'profit': 74, 'taking': 75, 'set': 76, 'two': 77, 'day': 78, 'gain': 79, 'dollar': 80, 'ahead': 81, 'comment': 82, 'fed': 83, 'chief': 84, 'alan': 85, 'expected': 86, 'case': 87, 'u': 88, 'rate': 89, 'rise': 90, 'say': 91, 'pulled': 92, 'deficit': 93, 'fiscal': 94, 'current': 95, 'account': 96, 'low': 97, 'interest': 98, 'driving': 99, 'greenback': 100, 'currency': 101, 'difficult': 102, 'predict': 103, 'fast': 104, 'far': 105, 'go': 106, 'bank': 107, 'canada': 108, 'governor': 109, 'david': 110, 'motorola': 111, 'wireless': 112, 'network': 113, 'developer': 114, 'eye': 115, 'defense': 116, 'contract': 117, 'chicago': 118, 'inc': 119, 'acquiring': 120, 'wi': 121, 'fi': 122, 'based': 123, 'technology': 124, 'help': 125, 'land': 126, 'growing': 127, 'government': 128, 'business': 129, 'shake': 130, 'retailer': 131, 'london': 132, 'mark': 133, 'amp': 134, 'announced': 135, 'management': 136, 'tuesday': 137, 'cost': 138, 'six': 139, 'senior': 140, 'executive': 141, 'job': 142, 'plunged': 143, 'sale': 144, 'home': 145, 'first': 146, 'half': 147, 'report': 148, 'mass': 149, 'wind': 150, 'farm': 151, 'power': 152, 'plant': 153, 'proposed': 154, 'water': 155, 'south': 156, 'cape': 157, 'major': 158, 'harm': 159, 'bird': 160, 'marine': 161, 'life': 162, 'could': 163, 'lead': 164, 'significant': 165, 'public': 166, 'health': 167, 'benefit': 168, 'peoplesoft': 169, 'get': 170, 'closer': 171, 'ibm': 172, 'threat': 173, 'hostile': 174, 'takeover': 175, 'oracle': 176, 'bn': 177, 'partnership': 178, 'speaking': 179, 'user': 180, 'conference': 181, 'san': 182, 'francisco': 183, 'yesterday': 184, 'santander': 185, 'union': 186, 'fear': 187, 'abbey': 188, 'chairman': 189, 'central': 190, 'spanish': 191, 'bidding': 192, 'national': 193, 'tell': 194, 'today': 195, 'successful': 196, 'acquisition': 197, 'result': 198, 'le': 199, 'dip': 200, 'warning': 201, 'sa': 202, 'share': 203, 'amid': 204, 'world': 205, 'second': 206, 'largest': 207, 'issue': 208, 'post': 209, 'third': 210, 'quarter': 211, 'revenue': 212, 'later': 213, 'britain': 214, 'jobless': 215, 'new': 216, 'unemployment': 217, 'fell': 218, 'latest': 219, 'according': 220, 'official': 221, 'data': 222, 'released': 223, 'way': 224, 'split': 225, 'unit': 226, 'group': 227, 'louis': 228, 'c': 229, 'might': 230, 'independent': 231, 'construction': 232, 'industrial': 233, 'give': 234, 'mixed': 235, 'signal': 236, 'spending': 237, 'surged': 238, 'august': 239, 'highest': 240, 'level': 241, 'record': 242, 'manufacturing': 243, 'grew': 244, 'slower': 245, 'pace': 246, 'september': 247, 'offering': 248, 'strength': 249, 'hire': 250, 'engineer': 251, 'japan': 252, 'planned': 253, 'newspaper': 254, 'push': 255, 'backed': 256, 'investment': 257, 'million': 258, 'euro': 259, 'nikkei': 260, 'close': 261, 'tokyo': 262, 'average': 263, 'rose': 264, 'four': 265, 'hope': 266, 'rally': 267, 'buying': 268, 'wide': 269, 'range': 270, 'tech': 271, 'extend': 272, 'asia': 273, 'crude': 274, 'oil': 275, 'price': 276, 'near': 277, 'barrel': 278, 'key': 279, 'economic': 280, 'auto': 281, 'sept': 282, 'soared': 283, 'led': 284, 'surge': 285, 'motor': 286, 'increase': 287, 'chrysler': 288, 'toyota': 289, 'gm': 290, 'boosting': 291, 'rebate': 292, 'wall': 293, 'street': 294, 'focus': 295, 'return': 296, 'york': 297, 'loss': 298, 'investor': 299, 'rising': 300, 'strong': 301, 'lehman': 302, 'brother': 303, 'sprint': 304, 'nextel': 305, 'communication': 306, 'air': 307, 'asks': 308, 'court': 309, 'labor': 310, 'airway': 311, 'asked': 312, 'throw': 313, 'passenger': 314, 'service': 315, 'agent': 316, 'flight': 317, 'worker': 318, 'replace': 319, 'expensive': 320, 'reliance': 321, 'india': 322, 'mumbai': 323, 'nov': 324, 'indian': 325, 'held': 326, 'family': 327, 'industry': 328, 'country': 329, 'expert': 330, 'talk': 331, 'yet': 332, 'month': 333, 'prison': 334, 'frank': 335, 'banking': 336, 'dot': 337, 'com': 338, 'boom': 339, 'sentenced': 340, 'federal': 341, 'camp': 342, 'calif': 343, 'still': 344, 'eight': 345, 'outside': 346, 'remain': 347, 'without': 348, 'deal': 349, 'scheduled': 350, 'none': 351, 'since': 352, 'june': 353, 'city': 354, 'manager': 355, 'jeff': 356, 'arrived': 357, 'nike': 358, 'founder': 359, 'step': 360, 'bos': 361, 'phil': 362, 'knight': 363, 'last': 364, 'night': 365, 'decision': 366, 'helped': 367, 'found': 368, 'ago': 369, 'buyer': 370, 'choose': 371, 'mcdonald': 372, 'christmas': 373, 'tree': 374, 'fake': 375, 'box': 376, 'hard': 377, 'put': 378, 'right': 379, 'american': 380, 'eagle': 381, 'reach': 382, 'pilot': 383, 'leader': 384, 'representing': 385, 'division': 386, 'airline': 387, 'accepted': 388, 'tentative': 389, 'agreement': 390, 'includes': 391, 'pay': 392, 'hit': 393, 'top': 394, 'increasingly': 395, 'worried': 396, 'impact': 397, 'surging': 398, 'consumer': 399, 'survey': 400, 'sunday': 401, 'showed': 402, 'verizon': 403, 'swap': 404, 'withdrew': 405, 'opposition': 406, 'quote': 407, 'profile': 408, 'research': 409, 'valuable': 410, 'europe': 411, 'clash': 412, 'subsidy': 413, 'boeing': 414, 'airbus': 415, 'fierce': 416, 'competition': 417, 'giant': 418, 'aircraft': 419, 'maker': 420, 'heating': 421, 'european': 422, 'stepped': 423, 'boost': 424, 'prospect': 425, 'save': 426, 'rescue': 427, 'try': 428, 'stop': 429, 'keep': 430, 'larger': 431, 'customer': 432, 'check': 433, 'point': 434, 'equity': 435, 'maintained': 436, 'rating': 437, 'target': 438, 'software': 439, 'nasdaq': 440, 'news': 441, 'people': 442, 'suggested': 443, 'although': 444, 'wait': 445, 'google': 446, 'debut': 447, 'make': 448, 'anticipated': 449, 'initial': 450, 'priced': 451, 'estimate': 452, 'raising': 453, 'ever': 454, 'grey': 455, 'global': 456, 'sir': 457, 'martin': 458, 'placed': 459, 'position': 460, 'rival': 461, 'advertising': 462, 'yahoo': 463, 'adding': 464, 'search': 465, 'engine': 466, 'tool': 467, 'following': 468, 'recent': 469, 'trend': 470, 'internet': 471, 'create': 472, 'personal': 473, 'favorite': 474, 'link': 475, 'others': 476, 'settle': 477, 'part': 478, 'pension': 479, 'lawsuit': 480, 'international': 481, 'machine': 482, 'corp': 483, 'least': 484, 'claimed': 485, 'firm': 486, 'older': 487, 'spin': 488, 'information': 489, 'system': 490, 'whose': 491, 'trade': 492, 'access': 493, 'judge': 494, 'microsoft': 495, 'e': 496, 'mail': 497, 'burst': 498, 'evidence': 499, 'antitrust': 500, 'suit': 501, 'july': 502, 'output': 503, 'factory': 504, 'run': 505, 'faster': 506, 'washington': 507, 'advanced': 508, 'capacity': 509, 'reserve': 510, 'weighs': 511, 'gdp': 512, 'yen': 513, 'surprising': 514, 'japanese': 515, 'growth': 516, 'worry': 517, 'paper': 518, 'loses': 519, 'loan': 520, 'jet': 521, 'lost': 522, 'financing': 523, 'nearly': 524, 'regional': 525, 'bankrupt': 526, 'reported': 527, 'saturday': 528, 'high': 529, 'struggling': 530, 'zealand': 531, 'dramatic': 532, 'usc': 533, 'halliburton': 534, 'may': 535, 'shed': 536, 'big': 537, 'operator': 538, 'iraq': 539, 'drop': 540, 'monday': 541, 'continued': 542, 'producer': 543, 'opec': 544, 'coming': 545, 'tight': 546, 'iraqi': 547, 'export': 548, 'insurer': 549, 'disputed': 550, 'fee': 551, 'insurance': 552, 'named': 553, 'attorney': 554, 'investigation': 555, 'questionable': 556, 'practice': 557, 'use': 558, 'incentive': 559, 'center': 560, 'probe': 561, 'battle': 562, 'seen': 563, 'getting': 564, 'control': 565, 'house': 566, 'turn': 567, 'full': 568, 'ruling': 569, 'sweet': 570, 'victory': 571, 'sugar': 572, 'hailed': 573, 'organisation': 574, 'rule': 575, 'east': 576, 'continue': 577, 'drive': 578, 'next': 579, 'fastest': 580, 'region': 581, 'reached': 582, 'peak': 583, 'expansion': 584, 'picture': 585, 'show': 586, 'rank': 587, 'working': 588, 'film': 589, 'dvd': 590, 'casino': 591, 'rock': 592, 'cafe': 593, 'boston': 594, 'scientific': 595, 'n': 596, 'already': 597, 'recovered': 598, 'popular': 599, 'heart': 600, 'device': 601, 'recall': 602, 'jones': 603, 'mln': 604, 'mid': 605, 'clothing': 606, 'href': 607, 'quickinfo': 608, 'fullquote': 609, 'worth': 610, 'move': 611, 'expand': 612, 'luxury': 613, 'human': 614, 'deeper': 615, 'q': 616, 'science': 617, 'posted': 618, 'wider': 619, 'hurt': 620, 'charge': 621, 'location': 622, 'retirement': 623, 'officer': 624, 'satellite': 625, 'television': 626, 'controlling': 627, 'ltd': 628, 'stake': 629, 'sky': 630, 'latin': 631, 'america': 632, 'combine': 633, 'operation': 634, 'spend': 635, 'retail': 636, 'telling': 637, 'platform': 638, 'completed': 639, 'driver': 640, 'annual': 641, 'agrees': 642, 'buyout': 643, 'united': 644, 'online': 645, 'california': 646, 'provider': 647, 'subscription': 648, 'agreed': 649, 'networking': 650, 'change': 651, 'certain': 652, 'nortel': 653, 'accounting': 654, 'reaching': 655, 'equipment': 656, 'aim': 657, 'id': 658, 'phone': 659, 'number': 660, 'order': 661, 'telephone': 662, 'identity': 663, 'airplane': 664, 'vulnerable': 665, 'bomber': 666, 'despite': 667, 'urging': 668, 'congress': 669, 'commission': 670, 'commercial': 671, 'suicide': 672, 'aviation': 673, 'security': 674, 'sought': 675, 'fda': 676, 'head': 677, 'senate': 678, 'finance': 679, 'committee': 680, 'called': 681, 'department': 682, 'launch': 683, 'allegation': 684, 'food': 685, 'drug': 686, 'administration': 687, 'went': 688, 'toshiba': 689, 'damage': 690, 'cbs': 691, 'mw': 692, 'field': 693, 'korea': 694, 'semiconductor': 695, 'state': 696, 'alleging': 697, 'infringement': 698, 'flash': 699, 'memory': 700, 'patent': 701, 'putin': 702, 'yukos': 703, 'asset': 704, 'russian': 705, 'president': 706, 'vladimir': 707, 'bid': 708, 'collect': 709, 'tax': 710, 'nation': 711, 'gas': 712, 'hold': 713, 'supply': 714, 'let': 715, 'import': 716, 'gulf': 717, 'offshore': 718, 'hurricane': 719, 'ivan': 720, 'uk': 721, 'emerged': 722, 'slump': 723, 'cisco': 724, 'pressure': 725, 'sent': 726, 'overall': 727, 'little': 728, 'changed': 729, 'widely': 730, 'hike': 731, 'cut': 732, 'forecast': 733, 'earnings': 734, 'ice': 735, 'cold': 736, 'drink': 737, 'product': 738, 'slowed': 739, 'war': 740, 'decade': 741, 'struggle': 742, 'france': 743, 'negotiator': 744, 'meet': 745, 'wto': 746, 'met': 747, 'organization': 748, 'formal': 749, 'series': 750, 'discussion': 751, 'ranging': 752, 'electronic': 753, 'delay': 754, 'navy': 755, 'audit': 756, 'seller': 757, 'computer': 758, 'delayed': 759, 'release': 760, 'review': 761, 'problem': 762, 'trial': 763, 'triggered': 764, 'safety': 765, 'best': 766, 'selling': 767, 'linked': 768, 'death': 769, 'injury': 770, 'clinical': 771, 'strike': 772, 'halt': 773, 'tea': 774, 'production': 775, 'north': 776, 'eastern': 777, 'brings': 778, 'work': 779, 'garden': 780, 'chain': 781, 'saved': 782, 'chip': 783, 'faced': 784, 'mountain': 785, 'inventory': 786, 'quick': 787, 'thinking': 788, 'manufacturer': 789, 'avoid': 790, 'began': 791, 'rd': 792, 'intel': 793, 'financial': 794, 'showing': 795, 'continues': 796, 'sell': 797, 'stockpile': 798, 'remains': 799, 'slow': 800, 'montreal': 801, 'served': 802, 'disappointing': 803, 'summer': 804, 'saying': 805, 'brazil': 806, 'ceo': 807, 'recovery': 808, 'harry': 809, 'speed': 810, 'fair': 811, 'waste': 812, 'free': 813, 'cash': 814, 'cover': 815, 'previous': 816, 'mistake': 817, 'look': 818, 'flat': 819, 'future': 820, 'open': 821, 'rush': 822, 'corporate': 823, 'ahold': 824, 'dutch': 825, 'prosecutor': 826, 'amsterdam': 827, 'settlement': 828, 'publishing': 829, 'false': 830, 'ended': 831, 'spate': 832, 'development': 833, 'including': 834, 'deadly': 835, 'attack': 836, 'saudi': 837, 'arabia': 838, 'adviser': 839, 'role': 840, 'ovitz': 841, 'draft': 842, 'employment': 843, 'walt': 844, 'disney': 845, 'michael': 846, 'testified': 847, 'jury': 848, 'tennessee': 849, 'ordered': 850, 'dispute': 851, 'bill': 852, 'designed': 853, 'counter': 854, 'counterfeit': 855, 'green': 856, 'touch': 857, 'blue': 858, 'red': 859, 'image': 860, 'star': 861, 'wave': 862, 'need': 863, 'ad': 864, 'cfo': 865, 'publisher': 866, 'see': 867, 'pick': 868, 'fourth': 869, 'nissan': 870, 'fall': 871, 'exporter': 872, 'slide': 873, 'trading': 874, 'eased': 875, 'concern': 876, 'school': 877, 'college': 878, 'medicine': 879, 'grand': 880, 'rapid': 881, 'university': 882, 'dreamworks': 883, 'shrek': 884, 'animation': 885, 'bell': 886, 'decided': 887, 'original': 888, 'date': 889, 'november': 890, 'sink': 891, 'due': 892, 'opening': 893, 'away': 894, 'ok': 895, 'treat': 896, 'cause': 897, 'painful': 898, 'related': 899, 'woman': 900, 'regulatory': 901, 'slightly': 902, 'barely': 903, 'suggesting': 904, 'inflation': 905, 'currently': 906, 'stick': 907, 'approach': 908, 'airport': 909, 'bought': 910, 'net': 911, 'men': 912, 'almost': 913, 'denies': 914, 'backing': 915, 'eu': 916, 'insisted': 917, 'intention': 918, 'side': 919, 'even': 920, 'though': 921, 'filed': 922, 'brief': 923, 'tag': 924, 'fire': 925, 'protection': 926, 'indicated': 927, 'prepared': 928, 'corporation': 929, 'conglomerate': 930, 'october': 931, 'massachusetts': 932, 'picked': 933, 'improving': 934, 'performance': 935, 'board': 936, 'option': 937, 'counted': 938, 'expense': 939, 'begin': 940, 'warns': 941, 'toronto': 942, 'warned': 943, 'preliminary': 944, 'lobby': 945, 'b': 946, 'vancouver': 947, 'cp': 948, 'needed': 949, 'recover': 950, 'duty': 951, 'canadian': 952, 'seven': 953, 'activity': 954, 'battered': 955, 'area': 956, 'southeast': 957, 'shopper': 958, 'march': 959, 'commerce': 960, 'came': 961, 'jump': 962, 'purchased': 963, 'houston': 964, 'medical': 965, 'expressed': 966, 'citigroup': 967, 'ex': 968, 'exec': 969, 'face': 970, 'sec': 971, 'action': 972, 'regulator': 973, 'enforcement': 974, 'proceeding': 975, 'former': 976, 'th': 977, 'unchanged': 978, 'straight': 979, 'risk': 980, 'soaring': 981, 'mining': 982, 'resource': 983, 'privately': 984, 'snow': 985, 'treasury': 986, 'secretary': 987, 'bush': 988, 'john': 989, 'stay': 990, 'ending': 991, 'loyal': 992, 'foot': 993, 'soldier': 994, 'replaced': 995, 'realnetworks': 996, 'download': 997, 'cent': 998, 'seattle': 999, 'per': 1000, 'song': 1001, 'music': 1002, 'place': 1003, 'medium': 1004, 'movie': 1005, 'snap': 1006, 'netflix': 1007, 'tivo': 1008, 'discus': 1009, 'downloads': 1010, 'staff': 1011, 'writer': 1012, 'ground': 1013, 'silicon': 1014, 'valley': 1015, 'alliance': 1016, 'owner': 1017, 'ran': 1018, 'store': 1019, 'owns': 1020, 'supermarket': 1021, 'throughout': 1022, 'northeast': 1023, 'hired': 1024, 'possible': 1025, 'ruled': 1026, 'black': 1027, 'gold': 1028, 'coke': 1029, 'soft': 1030, 'coca': 1031, 'cola': 1032, 'immediate': 1033, 'improved': 1034, 'brand': 1035, 'chemical': 1036, 'operating': 1037, 'support': 1038, 'index': 1039, 'higher': 1040, 'listed': 1041, 'sector': 1042, 'delta': 1043, 'slash': 1044, 'line': 1045, 'unveiled': 1046, 'carrier': 1047, 'bankruptcy': 1048, 'traffic': 1049, 'recovering': 1050, 'increasing': 1051, 'worldwide': 1052, 'transport': 1053, 'association': 1054, 'ailing': 1055, 'charlie': 1056, 'cancer': 1057, 'resigned': 1058, 'disease': 1059, 'cingular': 1060, 'cell': 1061, 'venture': 1062, 'cellphone': 1063, 'idea': 1064, 'cellular': 1065, 'malaysian': 1066, 'joint': 1067, 'employee': 1068, 'cluster': 1069, 'prepares': 1070, 'square': 1071, 'space': 1072, 'floor': 1073, 'building': 1074, 'st': 1075, 'bay': 1076, 'nz': 1077, 'f': 1078, 'p': 1079, 'h': 1080, 'hedge': 1081, 'dow': 1082, 'confident': 1083, 'better': 1084, 'associate': 1085, 'pleads': 1086, 'guilty': 1087, 'kumar': 1088, 'ny': 1089, 'pleaded': 1090, 'journal': 1091, 'afternoon': 1092, 'citing': 1093, 'source': 1094, 'approve': 1095, 'proposal': 1096, 'brought': 1097, 'failed': 1098, 'clear': 1099, 'sue': 1100, 'resistance': 1101, 'poised': 1102, 'predicted': 1103, 'winter': 1104, 'announcing': 1105, 'merck': 1106, 'plunge': 1107, 'vioxx': 1108, 'document': 1109, 'pharmaceutical': 1110, 'denied': 1111, 'atlanta': 1112, 'relief': 1113, 'hometown': 1114, 'mile': 1115, 'flying': 1116, 'relieved': 1117, 'hear': 1118, 'managed': 1119, 'workforce': 1120, 'station': 1121, 'material': 1122, 'exchange': 1123, 'aol': 1124, 'marketing': 1125, 'provide': 1126, 'antivirus': 1127, 'subscriber': 1128, 'slip': 1129, 'ranking': 1130, 'slipped': 1131, 'conducted': 1132, 'forum': 1133, 'declined': 1134, 'five': 1135, 'effort': 1136, 'private': 1137, 'capital': 1138, 'real': 1139, 'estate': 1140, 'property': 1141, 'pentagon': 1142, 'study': 1143, 'find': 1144, 'total': 1145, 'awarded': 1146, 'basis': 1147, 'gap': 1148, 'prompted': 1149, 'economist': 1150, 'warn': 1151, 'briefly': 1152, 'likely': 1153, 'moving': 1154, 'encouraged': 1155, 'bargain': 1156, 'among': 1157, 'agency': 1158, 'pennsylvania': 1159, 'turnpike': 1160, 'thousand': 1161, 'holiday': 1162, 'traveler': 1163, 'toll': 1164, 'surprised': 1165, 'many': 1166, 'watcher': 1167, 'everyone': 1168, 'convinced': 1169, 'seems': 1170, 'chipmaker': 1171, 'looking': 1172, 'plasma': 1173, 'tv': 1174, 'ap': 1175, 'hanging': 1176, 'care': 1177, 'soon': 1178, 'hang': 1179, 'inch': 1180, 'screen': 1181, 'develop': 1182, 'short': 1183, 'emergency': 1184, 'draw': 1185, 'prepare': 1186, 'main': 1187, 'producing': 1188, 'toy': 1189, 'january': 1190, 'amazon': 1191, 'team': 1192, 'fraud': 1193, 'filing': 1194, 'several': 1195, 'spammer': 1196, 'owen': 1197, 'illinois': 1198, 'clean': 1199, 'fund': 1200, 'given': 1201, 'generally': 1202, 'danger': 1203, 'blockbuster': 1204, 'hollywood': 1205, 'involving': 1206, 'campaign': 1207, 'video': 1208, 'parent': 1209, 'turned': 1210, 'consecutive': 1211, 'marsh': 1212, 'mclennan': 1213, 'resigns': 1214, 'eliot': 1215, 'spitzer': 1216, 'accused': 1217, 'broker': 1218, 'rigging': 1219, 'trump': 1220, 'broke': 1221, 'reality': 1222, 'author': 1223, 'crack': 1224, 'donald': 1225, 'flow': 1226, 'time': 1227, 'seal': 1228, 'fix': 1229, 'overnight': 1230, 'another': 1231, 'causing': 1232, 'german': 1233, 'stalled': 1234, 'mine': 1235, 'utility': 1236, 'restructure': 1237, 'antidepressant': 1238, 'want': 1239, 'learn': 1240, 'young': 1241, 'call': 1242, 'label': 1243, 'analysis': 1244, 'suspicious': 1245, 'around': 1246, 'volume': 1247, 'police': 1248, 'suggests': 1249, 'told': 1250, 'successor': 1251, 'pain': 1252, 'reliever': 1253, 'warner': 1254, 'aside': 1255, 'potential': 1256, 'fine': 1257, 'finish': 1258, 'finally': 1259, 'legal': 1260, 'strife': 1261, 'promise': 1262, 'germany': 1263, 'worst': 1264, 'view': 1265, 'fixing': 1266, 'crowd': 1267, 'emerge': 1268, 'different': 1269, 'battling': 1270, 'balance': 1271, 'opinion': 1272, 'thing': 1273, 'stand': 1274, 'success': 1275, 'cow': 1276, 'lg': 1277, 'electronics': 1278, 'file': 1279, 'measure': 1280, 'venezuelan': 1281, 'vote': 1282, 'venezuela': 1283, 'keeping': 1284, 'hugo': 1285, 'chavez': 1286, 'office': 1287, 'charged': 1288, 'electoral': 1289, 'viacom': 1290, 'circuit': 1291, 'shift': 1292, 'reduced': 1293, 'climbed': 1294, 'toward': 1295, 'considered': 1296, 'drawing': 1297, 'disrupted': 1298, 'left': 1299, 'small': 1300, 'billed': 1301, 'guy': 1302, 'hand': 1303, 'party': 1304, 'mci': 1305, 'creditor': 1306, 'member': 1307, 'considering': 1308, 'civil': 1309, 'press': 1310, 'cutting': 1311, 'facility': 1312, 'negative': 1313, 'site': 1314, 'rallied': 1315, 'late': 1316, 'decline': 1317, 'edge': 1318, 'manage': 1319, 'beat': 1320, 'used': 1321, 'child': 1322, 'porn': 1323, 'criminal': 1324, 'threaten': 1325, 'send': 1326, 'flood': 1327, 'pornography': 1328, 'come': 1329, 'gambling': 1330, 'website': 1331, 'berlin': 1332, 'double': 1333, 'trouble': 1334, 'younger': 1335, 'woe': 1336, 'social': 1337, 'quality': 1338, 'shopping': 1339, 'launched': 1340, 'compare': 1341, 'feature': 1342, 'alert': 1343, 'hollinger': 1344, 'take': 1345, 'circulation': 1346, 'issued': 1347, 'sun': 1348, 'acquire': 1349, 'automated': 1350, 'directory': 1351, 'voice': 1352, 'program': 1353, 'additional': 1354, 'consideration': 1355, 'common': 1356, 'art': 1357, 'recognition': 1358, 'qantas': 1359, 'plc': 1360, 'debt': 1361, 'weekly': 1362, 'headed': 1363, 'halted': 1364, 'versus': 1365, 'employer': 1366, 'seemed': 1367, 'willing': 1368, 'act': 1369, 'speaker': 1370, 'thomas': 1371, 'biotechnology': 1372, 'council': 1373, 'count': 1374, 'earlier': 1375, 'quit': 1376, 'agenda': 1377, 'russia': 1378, 'verge': 1379, 'stronger': 1380, 'relationship': 1381, 'becomes': 1382, 'route': 1383, 'pair': 1384, 'blocked': 1385, 'enron': 1386, 'permission': 1387, 'remaining': 1388, 'natural': 1389, 'pipeline': 1390, 'military': 1391, 'robust': 1392, 'reporting': 1393, 'dangerous': 1394, 'anti': 1395, 'broadcaster': 1396, 'australia': 1397, 'la': 1398, 'bridge': 1399, 'highway': 1400, 'think': 1401, 'imagine': 1402, 'swung': 1403, 'west': 1404, 'port': 1405, 'louisiana': 1406, 'signed': 1407, 'purchase': 1408, 'settled': 1409, 'agreeing': 1410, 'justice': 1411, 'favour': 1412, 'questioned': 1413, 'commissioner': 1414, 'made': 1415, 'appeared': 1416, 'positive': 1417, 'note': 1418, 'early': 1419, 'offset': 1420, 'alcoa': 1421, 'rejected': 1422, 'attempt': 1423, 'block': 1424, 'interview': 1425, 'ba': 1426, 'sydney': 1427, 'confirmed': 1428, 'steady': 1429, 'dealer': 1430, 'figure': 1431, 'deutsche': 1432, 'barcelona': 1433, 'reject': 1434, 'cd': 1435, 'longer': 1436, 'democratic': 1437, 'coalition': 1438, 'stumble': 1439, 'shipment': 1440, 'flu': 1441, 'vaccine': 1442, 'lot': 1443, 'standard': 1444, 'fuel': 1445, 'became': 1446, 'hurting': 1447, 'jaguar': 1448, 'ford': 1449, 'car': 1450, 'voted': 1451, 'scale': 1452, 'spark': 1453, 'outlook': 1454, 'mortgage': 1455, 'sparking': 1456, 'broad': 1457, 'cable': 1458, 'via': 1459, 'rental': 1460, 'grows': 1461, 'sentiment': 1462, 'round': 1463, 'increased': 1464, 'imf': 1465, 'afp': 1466, 'facing': 1467, 'threatened': 1468, 'factor': 1469, 'opened': 1470, 'meeting': 1471, 'track': 1472, 'malaysia': 1473, 'tap': 1474, 'join': 1475, 'vow': 1476, 'boycott': 1477, 'hotel': 1478, 'bad': 1479, 'mayor': 1480, 'good': 1481, 'locked': 1482, 'cooling': 1483, 'period': 1484, 'extending': 1485, 'bitter': 1486, 'receives': 1487, 'received': 1488, 'philadelphia': 1489, 'comcast': 1490, 'troubled': 1491, 'chapter': 1492, 'mobile': 1493, 'hong': 1494, 'kong': 1495, 'base': 1496, 'solid': 1497, 'touched': 1498, 'deadline': 1499, 'partner': 1500, 'pm': 1501, 'withdraw': 1502, 'australian': 1503, 'inquiry': 1504, 'volatile': 1505, 'significantly': 1506, 'ace': 1507, 'sears': 1508, 'shot': 1509, 'trust': 1510, 'disclosed': 1511, 'holding': 1512, 'climb': 1513, 'yield': 1514, 'lowest': 1515, 'marketwatch': 1516, 'acquired': 1517, 'paris': 1518, 'ipo': 1519, 'consider': 1520, 'ownership': 1521, 'suspends': 1522, 'suspended': 1523, 'distribution': 1524, 'unless': 1525, 'leading': 1526, 'closely': 1527, 'watched': 1528, 'climate': 1529, 'hewlett': 1530, 'packard': 1531, 'ipod': 1532, 'highlight': 1533, 'miami': 1534, 'branded': 1535, 'player': 1536, 'introduced': 1537, 'revamped': 1538, 'lineup': 1539, 'digital': 1540, 'cloud': 1541, 'automaker': 1542, 'automotive': 1543, 'single': 1544, 'credit': 1545, 'suisse': 1546, 'cautious': 1547, 'regarding': 1548, 'nyse': 1549, 'sony': 1550, 'mgm': 1551, 'consortium': 1552, 'principle': 1553, 'studio': 1554, 'metro': 1555, 'goldwyn': 1556, 'wal': 1557, 'mart': 1558, 'contribution': 1559, 'defeat': 1560, 'ballot': 1561, 'los': 1562, 'angeles': 1563, 'cheaper': 1564, 'poland': 1565, 'adam': 1566, 'ag': 1567, 'paul': 1568, 'expects': 1569, 'stephen': 1570, 'spokesman': 1571, 'trying': 1572, 'striking': 1573, 'olympic': 1574, 'greece': 1575, 'silver': 1576, 'fill': 1577, 'budget': 1578, 'hole': 1579, 'costly': 1580, 'athens': 1581, 'olympics': 1582, 'greek': 1583, 'defence': 1584, 'separate': 1585, 'behind': 1586, 'peer': 1587, 'riding': 1588, 'across': 1589, 'portfolio': 1590, 'claim': 1591, 'korean': 1592, 'provided': 1593, 'illegal': 1594, 'ministry': 1595, 'foreign': 1596, 'affair': 1597, 'urge': 1598, 'jakarta': 1599, 'taken': 1600, 'unprecedented': 1601, 'bring': 1602, 'g': 1603, 'fails': 1604, 'write': 1605, 'completely': 1606, 'rich': 1607, 'club': 1608, 'advance': 1609, 'gained': 1610, 'shell': 1611, 'royal': 1612, 'formed': 1613, 'merger': 1614, 'netherlands': 1615, 'sharply': 1616, 'bonus': 1617, 'package': 1618, 'director': 1619, 'safe': 1620, 'seemingly': 1621, 'beginning': 1622, 'question': 1623, 'super': 1624, 'status': 1625, 'schedule': 1626, 'february': 1627, 'departure': 1628, 'charlotte': 1629, 'mini': 1630, 'fort': 1631, 'fla': 1632, 'effect': 1633, 'size': 1634, 'pct': 1635, 'telecommunication': 1636, 'bolster': 1637, 'nigerian': 1638, 'approves': 1639, 'nigeria': 1640, 'passed': 1641, 'resolution': 1642, 'asking': 1643, 'compensation': 1644, 'community': 1645, 'bae': 1646, 'going': 1647, 'smaller': 1648, 'sport': 1649, 'vehicle': 1650, 'spring': 1651, 'symantec': 1652, 'veritas': 1653, 'moved': 1654, 'joining': 1655, 'storage': 1656, 'v': 1657, 'shore': 1658, 'discount': 1659, 'well': 1660, 'breaking': 1661, 'took': 1662, 'territory': 1663, 'heightened': 1664, 'prove': 1665, 'northern': 1666, 'murdoch': 1667, 'rupert': 1668, 'win': 1669, 'buyback': 1670, 'controversial': 1671, 'known': 1672, 'fifth': 1673, 'cast': 1674, 'warm': 1675, 'weather': 1676, 'domestic': 1677, 'ticket': 1678, 'columnist': 1679, 'steven': 1680, 'column': 1681, 'jumped': 1682, 'driven': 1683, 'park': 1684, 'broadcast': 1685, 'edition': 1686, 'previously': 1687, 'express': 1688, 'referendum': 1689, 'newratings': 1690, 'craig': 1691, 'conway': 1692, 'arrives': 1693, 'hague': 1694, 'summit': 1695, 'gt': 1696, 'prime': 1697, 'minister': 1698, 'manmohan': 1699, 'singh': 1700, 'tonight': 1701, 'participate': 1702, 'strategic': 1703, 'aid': 1704, 'sensitive': 1705, 'gathered': 1706, 'presidential': 1707, 'election': 1708, 'pc': 1709, 'seat': 1710, 'hardware': 1711, 'opportunity': 1712, 'environment': 1713, 'unveils': 1714, 'arm': 1715, 'emc': 1716, 'fought': 1717, 'large': 1718, 'georgia': 1719, 'tender': 1720, 'offer': 1721, 'reduce': 1722, 'cleared': 1723, 'welcomed': 1724, 'game': 1725, 'halo': 1726, 'testing': 1727, 'tracking': 1728, 'spot': 1729, 'cabinet': 1730, 'reportedly': 1731, 'forward': 1732, 'december': 1733, 'ten': 1734, 'treatment': 1735, 'generate': 1736, 'shortage': 1737, 'revive': 1738, 'twice': 1739, 'web': 1740, 'start': 1741, 'crucial': 1742, 'season': 1743, 'reason': 1744, 'brazilian': 1745, 'george': 1746, 'w': 1747, 'challenger': 1748, 'kerry': 1749, 'agree': 1750, 'headquarters': 1751, 'probably': 1752, 'supported': 1753, 'hunting': 1754, 'sharp': 1755, 'gateway': 1756, 'integrate': 1757, 'governance': 1758, 'associated': 1759, 'ensure': 1760, 'serving': 1761, 'william': 1762, 'light': 1763, 'mercantile': 1764, 'prescription': 1765, 'always': 1766, 'rebound': 1767, 'got': 1768, 'lift': 1769, 'ottawa': 1770, 'disk': 1771, 'acting': 1772, 'suzuki': 1773, 'planning': 1774, 'diesel': 1775, 'assembly': 1776, 'book': 1777, 'harvard': 1778, 'institution': 1779, 'compete': 1780, 'like': 1781, 'seeking': 1782, 'refund': 1783, 'protected': 1784, 'law': 1785, 'widespread': 1786, 'corruption': 1787, 'cite': 1788, 'stage': 1789, 'telstra': 1790, 'offered': 1791, 'title': 1792, 'contender': 1793, 'appears': 1794, 'supercomputer': 1795, 'champion': 1796, 'fcc': 1797, 'granted': 1798, 'condition': 1799, 'bull': 1800, 'ease': 1801, 'terror': 1802, 'terrorism': 1803, 'debate': 1804, 'map': 1805, 'acquires': 1806, 'undisclosed': 1807, 'amount': 1808, 'rivalry': 1809, 'interstate': 1810, 'minority': 1811, 'special': 1812, 'overcome': 1813, 'marked': 1814, 'lackluster': 1815, 'hybrid': 1816, 'moment': 1817, 'announces': 1818, 'belarus': 1819, 'money': 1820, 'saddam': 1821, 'hussein': 1822, 'statement': 1823, 'followed': 1824, 'transaction': 1825, 'lesson': 1826, 'radical': 1827, 'nobel': 1828, 'prize': 1829, 'norwegian': 1830, 'born': 1831, 'edward': 1832, 'swedish': 1833, 'worse': 1834, 'along': 1835, 'slowdown': 1836, 'taiwan': 1837, 'island': 1838, 'indicating': 1839, 'tough': 1840, 'surprise': 1841, 'universal': 1842, 'french': 1843, 'collapsed': 1844, 'tribunal': 1845, 'gave': 1846, 'able': 1847, 'pursue': 1848, 'tie': 1849, 'population': 1850, 'retired': 1851, 'every': 1852, 'active': 1853, 'visa': 1854, 'supreme': 1855, 'card': 1856, 'violated': 1857, 'morgan': 1858, 'portion': 1859, 'ongoing': 1860, 'african': 1861, 'miner': 1862, 'harmony': 1863, 'failure': 1864, 'siege': 1865, 'shrine': 1866, 'uncertainty': 1867, 'dell': 1868, 'extended': 1869, 'optimistic': 1870, 'strategy': 1871, 'improve': 1872, 'culture': 1873, 'income': 1874, 'drove': 1875, 'session': 1876, 'trader': 1877, 'starting': 1878, 'brussels': 1879, 'demanded': 1880, 'explain': 1881, 'clearly': 1882, 'challenge': 1883, 'targeting': 1884, 'inter': 1885, 'spell': 1886, 'dsl': 1887, 'de': 1888, 'compared': 1889, 'un': 1890, 'reveals': 1891, 'especially': 1892, 'developing': 1893, 'closed': 1894, 'disruption': 1895, 'shi': 1896, 'ite': 1897, 'religious': 1898, 'peace': 1899, 'najaf': 1900, 'ontario': 1901, 'accuses': 1902, 'improper': 1903, 'mutual': 1904, 'qtr': 1905, 'expect': 1906, 'optimism': 1907, 'boosted': 1908, 'lifted': 1909, 'panel': 1910, 'advisory': 1911, 'minimum': 1912, 'required': 1913, 'produce': 1914, 'gasoline': 1915, 'labour': 1916, 'nokia': 1917, 'falling': 1918, 'regain': 1919, 'farmer': 1920, 'threatening': 1921, 'ivory': 1922, 'coast': 1923, 'protest': 1924, 'paid': 1925, 'indonesian': 1926, 'diplomat': 1927, 'susilo': 1928, 'yudhoyono': 1929, 'indonesia': 1930, 'quickly': 1931, 'form': 1932, 'l': 1933, 'modest': 1934, 'fueled': 1935, 'ask': 1936, 'jeeves': 1937, 'catch': 1938, 'hoping': 1939, 'shadow': 1940, 'visitor': 1941, 'page': 1942, 'destruction': 1943, 'caribbean': 1944, 'mexico': 1945, 'florida': 1946, 'alabama': 1947, 'housing': 1948, 'rice': 1949, 'supplier': 1950, 'kind': 1951, 'mich': 1952, 'gary': 1953, 'restaurant': 1954, 'truck': 1955, 'ball': 1956, 'prompt': 1957, 'daimlerchrysler': 1958, 'robin': 1959, 'guidance': 1960, 'seem': 1961, 'caused': 1962, 'army': 1963, 'simple': 1964, 'message': 1965, 'giving': 1966, 'roughly': 1967, 'stuff': 1968, 'also': 1969, 'r': 1970, 'wanted': 1971, 'direction': 1972, 'struggled': 1973, 'charley': 1974, 'slammed': 1975, 'solution': 1976, 'parmalat': 1977, 'grant': 1978, 'motion': 1979, 'remove': 1980, 'stopping': 1981, 'suing': 1982, 'italian': 1983, 'dairy': 1984, 'sued': 1985, 'nears': 1986, 'collapse': 1987, 'merrill': 1988, 'lynch': 1989, 'quits': 1990, 'clearing': 1991, 'eisner': 1992, 'choice': 1993, 'robert': 1994, 'stern': 1995, 'fired': 1996, 'man': 1997, 'greatest': 1998, 'independence': 1999, 'extension': 2000, 'ups': 2001, 'parcel': 2002, 'delivery': 2003, 'sending': 2004, 'telecom': 2005, 'racing': 2006, 'shrimp': 2007, 'appear': 2008, 'table': 2009, 'limited': 2010, 'engineering': 2011, 'federation': 2012, 'poor': 2013, 'expectation': 2014, 'foreigner': 2015, 'ebay': 2016, 'apartment': 2017, 'caution': 2018, 'whether': 2019, 'turnaround': 2020, 'compromise': 2021, 'plane': 2022, 'remark': 2023, 'investigating': 2024, 'painkiller': 2025, 'recently': 2026, 'ata': 2027, 'fare': 2028, 'lay': 2029, 'hundred': 2030, 'siemens': 2031, 'flaw': 2032, 'ring': 2033, 'newest': 2034, 'model': 2035, 'handset': 2036, 'understanding': 2037, 'decide': 2038, 'class': 2039, 'alleged': 2040, 'deputy': 2041, 'managing': 2042, 'editor': 2043, 'appointed': 2044, 'coverage': 2045, 'clinton': 2046, 'scandal': 2047, 'add': 2048, 'stun': 2049, 'gun': 2050, 'protect': 2051, 'approved': 2052, 'allows': 2053, 'connection': 2054, 'fly': 2055, 'nuclear': 2056, 'genuine': 2057, 'distributor': 2058, 'replacement': 2059, 'pushing': 2060, 'final': 2061, 'often': 2062, 'century': 2063, 'become': 2064, 'monthly': 2065, 'magazine': 2066, 'virtually': 2067, 'fossil': 2068, 'type': 2069, 'really': 2070, 'pound': 2071, 'pakistan': 2072, 'grow': 2073, 'payment': 2074, 'river': 2075, 'raw': 2076, 'progress': 2077, 'sam': 2078, 'buoyed': 2079, 'heavy': 2080, 'oct': 2081, 'gathering': 2082, 'guard': 2083, 'door': 2084, 'signing': 2085, 'msft': 2086, 'version': 2087, 'window': 2088, 'xp': 2089, 'instant': 2090, 'messaging': 2091, 'definition': 2092, 'living': 2093, 'room': 2094, 'feared': 2095, 'fedex': 2096, 'spread': 2097, 'colleague': 2098, 'massive': 2099, 'cool': 2100, 'gross': 2101, 'cincinnati': 2102, 'pact': 2103, 'grocery': 2104, 'local': 2105, 'doubt': 2106, 'kept': 2107, 'voter': 2108, 'poll': 2109, 'casualty': 2110, 'wonder': 2111, 'fannie': 2112, 'mae': 2113, 'scam': 2114, 'turning': 2115, 'jersey': 2116, 'pass': 2117, 'force': 2118, 'famous': 2119, 'rocked': 2120, 'generation': 2121, 'hot': 2122, 'developed': 2123, 'thanksgiving': 2124, 'travel': 2125, 'weekend': 2126, 'sponsor': 2127, 'kill': 2128, 'middle': 2129, 'continuing': 2130, 'nd': 2131, 'build': 2132, 'searching': 2133, 'apparent': 2134, 'brink': 2135, 'admitted': 2136, 'race': 2137, 'enough': 2138, 'lender': 2139, 'beyond': 2140, 'prominent': 2141, 'white': 2142, 'aide': 2143, 'tobacco': 2144, 'pfizer': 2145, 'arthritis': 2146, 'novel': 2147, 'allow': 2148, 'owned': 2149, 'body': 2150, 'putting': 2151, 'together': 2152, 'running': 2153, 'auction': 2154, 'overseas': 2155, 'remember': 2156, 'shelf': 2157, 'piracy': 2158, 'road': 2159, 'kansa': 2160, 'mo': 2161, 'aug': 2162, 'contractor': 2163, 'chase': 2164, 'chance': 2165, 'withdrawn': 2166, 'loom': 2167, 'within': 2168, 'lived': 2169, 'downgrade': 2170, 'surrounding': 2171, 'gene': 2172, 'honor': 2173, 'chart': 2174, 'secured': 2175, 'sustained': 2176, 'wild': 2177, 'extends': 2178, 'leaving': 2179, 'moscow': 2180, 'dozen': 2181, 'embattled': 2182, 'idc': 2183, 'petroleum': 2184, 'easing': 2185, 'creating': 2186, 'response': 2187, 'favor': 2188, 'jim': 2189, 'engaged': 2190, 'unfair': 2191, 'follow': 2192, 'jan': 2193, 'saving': 2194, 'invest': 2195, 'losing': 2196, 'break': 2197, 'fewer': 2198, 'promotion': 2199, 'old': 2200, 'lockout': 2201, 'created': 2202, 'strain': 2203, 'watching': 2204, 'modern': 2205, 'patient': 2206, 'ability': 2207, 'expands': 2208, 'niche': 2209, 'ready': 2210, 'posting': 2211, 'rolling': 2212, 'turner': 2213, 'disappointed': 2214, 'allegedly': 2215, 'arizona': 2216, 'possibly': 2217, 'bangalore': 2218, 'competitive': 2219, 'timing': 2220, 'penalty': 2221, 'newly': 2222, 'braced': 2223, 'withdrawal': 2224, 'adopted': 2225, 'entire': 2226, 'trademark': 2227, 'forcing': 2228, 'stable': 2229, 'teen': 2230, 'oriented': 2231, 'fashion': 2232, 'forced': 2233, 'bigger': 2234, 'ac': 2235, 'steel': 2236, 'slashed': 2237, 'person': 2238, 'stroke': 2239, 'huge': 2240, 'keyboard': 2241, 'emerging': 2242, 'institute': 2243, 'rocket': 2244, 'something': 2245, 'challenging': 2246, 'reception': 2247, 'sized': 2248, 'shark': 2249, 'enterprise': 2250, 'atmosphere': 2251, 'feel': 2252, 'wake': 2253, 'violating': 2254, 'advantage': 2255, 'apparently': 2256, 'greater': 2257, 'grocer': 2258, 'throwing': 2259, 'piece': 2260, 'officially': 2261, 'exclusive': 2262, 'metal': 2263, 'test': 2264, 'redmond': 2265, 'wine': 2266, 'shipping': 2267, 'heard': 2268, 'argument': 2269, 'bar': 2270, 'directly': 2271, 'illegally': 2272, 'fixed': 2273, 'server': 2274, 'watchdog': 2275, 'productivity': 2276, 'diego': 2277, 'winning': 2278, 'prompting': 2279, 'fresh': 2280, 'indictment': 2281, 'environmental': 2282, 'wage': 2283, 'hour': 2284, 'limit': 2285, 'usa': 2286, 'notice': 2287, 'failing': 2288, 'published': 2289, 'rogers': 2290, 'beating': 2291, 'fade': 2292, 'michigan': 2293, 'exactly': 2294, 'smoking': 2295, 'ban': 2296, 'specialist': 2297, 'barred': 2298, 'van': 2299, 'individual': 2300, 'broader': 2301, 'abuse': 2302, 'cnn': 2303, 'expanded': 2304, 'healthy': 2305, 'charles': 2306, 'prince': 2307, 'reputation': 2308, 'daniel': 2309, 'processing': 2310, 'grain': 2311, 'processor': 2312, 'quota': 2313, 'listing': 2314, 'appeal': 2315, 'micro': 2316, 'vega': 2317, 'entertainment': 2318, 'filled': 2319, 'atlantic': 2320, 'unexpected': 2321, 'athletic': 2322, 'shoe': 2323, 'weak': 2324, 'suggest': 2325, 'photo': 2326, 'added': 2327, 'apple': 2328, 'heavily': 2329, 'portable': 2330, 'oversight': 2331, 'display': 2332, 'wary': 2333, 'retire': 2334, 'rather': 2335, 'ride': 2336, 'bit': 2337, 'mob': 2338, 'violence': 2339, 'weapon': 2340, 'al': 2341, 'carried': 2342, 'spent': 2343, 'dominate': 2344, 'grab': 2345, 'headline': 2346, 'sirius': 2347, 'counting': 2348, 'howard': 2349, 'coup': 2350, 'radio': 2351, 'setback': 2352, 'traditional': 2353, 'bond': 2354, 'benchmark': 2355, 'turnover': 2356, 'improvement': 2357, 'kodak': 2358, 'weaker': 2359, 'camera': 2360, 'printing': 2361, 'multimedia': 2362, 'available': 2363, 'visit': 2364, 'story': 2365, 'saw': 2366, 'county': 2367, 'retain': 2368, 'presence': 2369, 'booming': 2370, 'selected': 2371, 'longtime': 2372, 'reform': 2373, 'district': 2374, 'evening': 2375, 'sitting': 2376, 'resident': 2377, 'violation': 2378, 'numerous': 2379, 'examine': 2380, 'auditor': 2381, 'bt': 2382, 'broadband': 2383, 'mill': 2384, 'nine': 2385, 'mitsubishi': 2386, 'renewed': 2387, 'commitment': 2388, 'project': 2389, 'tanker': 2390, 'dubai': 2391, 'pump': 2392, 'lifting': 2393, 'upbeat': 2394, 'policy': 2395, 'cleveland': 2396, 'northwest': 2397, 'concession': 2398, 'mull': 2399, 'technical': 2400, 'unclear': 2401, 'remained': 2402, 'injured': 2403, 'texas': 2404, 'eas': 2405, 'fallen': 2406, 'doubled': 2407, 'nv': 2408, 'repeat': 2409, 'past': 2410, 'x': 2411, 'present': 2412, 'da': 2413, 'silva': 2414, 'visited': 2415, 'abroad': 2416, 'thought': 2417, 'welcome': 2418, 'java': 2419, 'returning': 2420, 'microsystems': 2421, 'transportation': 2422, 'senator': 2423, 'republican': 2424, 'vowed': 2425, 'closing': 2426, 'k': 2427, 'detail': 2428, 'style': 2429, 'plus': 2430, 'reduction': 2431, 'detroit': 2432, 'word': 2433, 'asean': 2434, 'caught': 2435, 'forget': 2436, 'corner': 2437, 'norton': 2438, 'backup': 2439, 'accident': 2440, 'killed': 2441, 'struck': 2442, 'friend': 2443, 'survivor': 2444, 'making': 2445, 'layoff': 2446, 'manhattan': 2447, 'laden': 2448, 'branch': 2449, 'queen': 2450, 'subsidiary': 2451, 'kick': 2452, 'suffered': 2453, 'ousted': 2454, 'conrad': 2455, 'banker': 2456, 'sexual': 2457, 'missed': 2458, 'non': 2459, 'watch': 2460, 'survived': 2461, 'fatal': 2462, 'receiving': 2463, 'hospital': 2464, 'scotland': 2465, 'depot': 2466, 'helping': 2467, 'hd': 2468, 'rest': 2469, 'influence': 2470, 'command': 2471, 'training': 2472, 'commander': 2473, 'award': 2474, 'conflict': 2475, 'spike': 2476, 'crop': 2477, 'armed': 2478, 'inspector': 2479, 'investigate': 2480, 'mac': 2481, 'blow': 2482, 'devastating': 2483, 'retreat': 2484, 'anything': 2485, 'item': 2486, 'unexpectedly': 2487, 'hu': 2488, 'visiting': 2489, 'jintao': 2490, 'political': 2491, 'cultural': 2492, 'spain': 2493, 'complete': 2494, 'kmart': 2495, 'representative': 2496, 'vendor': 2497, 'slight': 2498, 'believe': 2499, 'handed': 2500, 'adobe': 2501, 'plug': 2502, 'format': 2503, 'updated': 2504, 'j': 2505, 'sealed': 2506, 'negotiation': 2507, 'deep': 2508, 'bounce': 2509, 'sold': 2510, 'started': 2511, 'everything': 2512, 'weigh': 2513, 'gear': 2514, 'nothing': 2515, 'traded': 2516, 'philip': 2517, 'lcd': 2518, 'exile': 2519, 'survival': 2520, 'curb': 2521, 'strengthen': 2522, 'western': 2523, 'pacific': 2524, 'claiming': 2525, 'name': 2526, 'vice': 2527, 'brewer': 2528, 'sao': 2529, 'paulo': 2530, 'belgium': 2531, 'blast': 2532, 'laboratory': 2533, 'king': 2534, 'mostly': 2535, 'dallas': 2536, 'flagship': 2537, 'internal': 2538, 'bear': 2539, 'inside': 2540, 'restriction': 2541, 'pledge': 2542, 'yuan': 2543, 'beijing': 2544, 'tour': 2545, 'dropped': 2546, 'crown': 2547, 'chirac': 2548, 'hail': 2549, 'trip': 2550, 'kennedy': 2551, 'busiest': 2552, 'england': 2553, 'possibility': 2554, 'semi': 2555, 'steve': 2556, 'ballmer': 2557, 'thomson': 2558, 'publication': 2559, 'golf': 2560, 'structure': 2561, 'temperature': 2562, 'interior': 2563, 'lowered': 2564, 'upon': 2565, 'arbitration': 2566, 'conduct': 2567, 'priority': 2568, 'crime': 2569, 'familiar': 2570, 'extra': 2571, 'severe': 2572, 'waited': 2573, 'existing': 2574, 'impose': 2575, 'sank': 2576, 'monetary': 2577, 'jp': 2578, 'cazenove': 2579, 'effectively': 2580, 'richard': 2581, 'accusing': 2582, 'southwest': 2583, 'turf': 2584, 'knock': 2585, 'investing': 2586, 'broken': 2587, 'concerned': 2588, 'pixar': 2589, 'scene': 2590, 'mr': 2591, 'fellow': 2592, 'introduce': 2593, 'legislation': 2594, 'student': 2595, 'age': 2596, 'province': 2597, 'awaits': 2598, 'outcome': 2599, 'replacing': 2600, 'powerful': 2601, 'linux': 2602, 'train': 2603, 'bayer': 2604, 'entered': 2605, 'patch': 2606, 'obstacle': 2607, 'leaf': 2608, 'live': 2609, 'heel': 2610, 'sea': 2611, 'upcoming': 2612, 'request': 2613, 'stem': 2614, 'library': 2615, 'front': 2616, 'runner': 2617, 'pressed': 2618, 'verisign': 2619, 'responsible': 2620, 'domain': 2621, 'freeze': 2622, 'controlled': 2623, 'intends': 2624, 'fate': 2625, 'largely': 2626, 'ireland': 2627, 'govt': 2628, 'either': 2629, 'continent': 2630, 'lack': 2631, 'statistic': 2632, 'aerospace': 2633, 'jeanne': 2634, 'tore': 2635, 'determine': 2636, 'daily': 2637, 'miller': 2638, 'kingdom': 2639, 'southeastern': 2640, 'delivered': 2641, 'holder': 2642, 'resort': 2643, 'james': 2644, 'succeed': 2645, 'weight': 2646, 'competing': 2647, 'dominant': 2648, 'theft': 2649, 'steal': 2650, 'justin': 2651, 'andreas': 2652, 'planet': 2653, 'mind': 2654, 'gaming': 2655, 'truce': 2656, 'militia': 2657, 'crm': 2658, 'focusing': 2659, 'exit': 2660, 'eliminate': 2661, 'cargo': 2662, 'convention': 2663, 'underway': 2664, 'soybean': 2665, 'ninth': 2666, 'infected': 2667, 'highly': 2668, 'rapidly': 2669, 'swept': 2670, 'history': 2671, 'explorer': 2672, 'easier': 2673, 'schwab': 2674, 'language': 2675, 'occurred': 2676, 'optical': 2677, 'thanks': 2678, 'temporary': 2679, 'oakland': 2680, 'wright': 2681, 'gone': 2682, 'usual': 2683, 'unlikely': 2684, 'consulting': 2685, 'pioneer': 2686, 'perhaps': 2687, 'letter': 2688, 'wife': 2689, 'teenage': 2690, 'lawyer': 2691, 'centre': 2692, 'disrupt': 2693, 'tension': 2694, 'forest': 2695, 'miss': 2696, 'goal': 2697, 'shock': 2698, 'application': 2699, 'cbc': 2700, 'involved': 2701, 'gate': 2702, 'calling': 2703, 'combined': 2704, 'jack': 2705, 'roll': 2706, 'unwanted': 2707, 'alitalia': 2708, 'accept': 2709, 'italy': 2710, 'sweeping': 2711, 'situation': 2712, 'swing': 2713, 'broadcasting': 2714, 'divided': 2715, 'pool': 2716, 'anil': 2717, 'ambani': 2718, 'rumour': 2719, 'elder': 2720, 'minnesota': 2721, 'license': 2722, 'thwart': 2723, 'slam': 2724, 'course': 2725, 'storm': 2726, 'blamed': 2727, 'vodafone': 2728, 'offensive': 2729, 'database': 2730, 'collins': 2731, 'stewart': 2732, 'libel': 2733, 'gov': 2734, 'friendly': 2735, 'powered': 2736, 'promised': 2737, 'design': 2738, 'self': 2739, 'pa': 2740, 'providing': 2741, 'secure': 2742, 'authority': 2743, 'pretty': 2744, 'sort': 2745, 'announce': 2746, 'revealed': 2747, 'address': 2748, 'enters': 2749, 'brad': 2750, 'td': 2751, 'row': 2752, 'abandon': 2753, 'easy': 2754, 'settling': 2755, 'racial': 2756, 'honda': 2757, 'urged': 2758, 'assist': 2759, 'crisis': 2760, 'effective': 2761, 'await': 2762, 'bellwether': 2763, 'earned': 2764, 'soar': 2765, 'austrian': 2766, 'competitor': 2767, 'va': 2768, 'list': 2769, 'fox': 2770, 'channel': 2771, 'bextra': 2772, 'skin': 2773, 'professional': 2774, 'writes': 2775, 'written': 2776, 'matter': 2777, 'passing': 2778, 'component': 2779, 'majority': 2780, 'whatever': 2781, 'happened': 2782, 'nature': 2783, 'pushed': 2784, 'drew': 2785, 'similar': 2786, 'suspend': 2787, 'addition': 2788, 'suspension': 2789, 'penny': 2790, 'distance': 2791, 'minute': 2792, 'portal': 2793, 'built': 2794, 'son': 2795, 'administrator': 2796, 'melbourne': 2797, 'au': 2798, 'unable': 2799, 'setting': 2800, 'cross': 2801, 'border': 2802, 'era': 2803, 'multi': 2804, 'produced': 2805, 'stance': 2806, 'tim': 2807, 'lee': 2808, 'passage': 2809, 'path': 2810, 'provision': 2811, 'aimed': 2812, 'upgrade': 2813, 'transmission': 2814, 'appliance': 2815, 'brokerage': 2816, 'escaped': 2817, 'affect': 2818, 'momentum': 2819, 'disc': 2820, 'iran': 2821, 'uranium': 2822, 'sanction': 2823, 'must': 2824, 'pre': 2825, 'instead': 2826, 'merge': 2827, 'expanding': 2828, 'defends': 2829, 'string': 2830, 'refusing': 2831, 'speech': 2832, 'lawmaker': 2833, 'salary': 2834, 'carmaker': 2835, 'combination': 2836, 'frankfurt': 2837, 'voting': 2838, 'eighth': 2839, 'adult': 2840, 'scare': 2841, 'event': 2842, 'hidden': 2843, 'selection': 2844, 'dream': 2845, 'crackdown': 2846, 'copyright': 2847, 'task': 2848, 'combat': 2849, 'swapping': 2850, 'beer': 2851, 'arab': 2852, 'committed': 2853, 'bringing': 2854, 'poverty': 2855, 'core': 2856, 'confusion': 2857, 'alarm': 2858, 'dial': 2859, 'southern': 2860, 'plenty': 2861, 'quiet': 2862, 'shown': 2863, 'reverse': 2864, 'cooperation': 2865, 'equal': 2866, 'pt': 2867, 'reign': 2868, 'ip': 2869, 'telephony': 2870, 'deployment': 2871, 'infrastructure': 2872, 'lukoil': 2873, 'munich': 2874, 'recorded': 2875, 'batting': 2876, 'wrong': 2877, 'actually': 2878, 'alone': 2879, 'tried': 2880, 'kid': 2881, 'mastermind': 2882, 'charity': 2883, 'banning': 2884, 'campbell': 2885, 'overtime': 2886, 'hearing': 2887, 'joseph': 2888, 'leadership': 2889, 'profitable': 2890, 'considers': 2891, 'laptop': 2892, 'cereal': 2893, 'whole': 2894, 'brown': 2895, 'chancellor': 2896, 'gordon': 2897, 'defended': 2898, 'mp': 2899, 'laid': 2900, 'del': 2901, 'ship': 2902, 'snapped': 2903, 'streak': 2904, 'anyone': 2905, 'buffalo': 2906, 'blackberry': 2907, 'bluetooth': 2908, 'us': 2909, 'mainstream': 2910, 'hype': 2911, 'interim': 2912, 'pose': 2913, 'banner': 2914, 'heat': 2915, 'raider': 2916, 'dissident': 2917, 'glazer': 2918, 'malcolm': 2919, 'gop': 2920, 'town': 2921, 'hiring': 2922, 'worked': 2923, 'comprehensive': 2924, 'promising': 2925, 'amd': 2926, 'predicts': 2927, 'ft': 2928, 'rome': 2929, 'bet': 2930, 'perth': 2931, 'aa': 2932, 'stepping': 2933, 'process': 2934, 'fighter': 2935, 'funding': 2936, 'sense': 2937, 'external': 2938, 'phillips': 2939, 'critical': 2940, 'play': 2941, 'ink': 2942, 'winner': 2943, 'celtic': 2944, 'potentially': 2945, 'zone': 2946, 'instrument': 2947, 'fan': 2948, 'trainer': 2949, 'crew': 2950, 'dropping': 2951, 'kicked': 2952, 'seeing': 2953, 'halifax': 2954, 'treated': 2955, 'asbestos': 2956, 'accusation': 2957, 'el': 2958, 'wrigley': 2959, 'sensor': 2960, 'veteran': 2961, 'researcher': 2962, 'society': 2963, 'fishing': 2964, 'deliver': 2965, 'lowe': 2966, 'heavyweight': 2967, 'threatens': 2968, 'pilgrim': 2969, 'fat': 2970, 'forbes': 2971, 'liberty': 2972, 'branson': 2973, 'virgin': 2974, 'using': 2975, 'acknowledged': 2976, 'reaction': 2977, 'preparing': 2978, 'trillion': 2979, 'assistant': 2980, 'serious': 2981, 'feeling': 2982, 'however': 2983, 'observer': 2984, 'proof': 2985, 'md': 2986, 'surgery': 2987, 'maryland': 2988, 'premium': 2989, 'sharing': 2990, 'incumbent': 2991, 'opponent': 2992, 'wing': 2993, 'defend': 2994, 'contest': 2995, 'dark': 2996, 'empty': 2997, 'conservative': 2998, 'foundation': 2999, 'becoming': 3000, 'never': 3001, 'know': 3002, 'love': 3003, 'sure': 3004, 'assessment': 3005, 'radar': 3006, 'advice': 3007, 'panic': 3008, 'jeremy': 3009, 'calm': 3010, 'milan': 3011, 'ally': 3012, 'important': 3013, 'secret': 3014, 'margin': 3015, 'waiting': 3016, 'swiss': 3017, 'coal': 3018, 'fail': 3019, 'disaster': 3020, 'taylor': 3021, 'sbc': 3022, 'girl': 3023, 'begun': 3024, 'weakness': 3025, 'stood': 3026, 'direct': 3027, 'sends': 3028, 'lewis': 3029, 'resumed': 3030, 'sandwich': 3031, 'sub': 3032, 'serve': 3033, 'answer': 3034, 'sixth': 3035, 'thai': 3036, 'columbus': 3037, 'ohio': 3038, 'mad': 3039, 'prevent': 3040, 'beef': 3041, 'distributed': 3042, 'paying': 3043, 'cap': 3044, 'shine': 3045, 'rare': 3046, 'spotlight': 3047, 'someone': 3048, 'bound': 3049, 'indicted': 3050, 'ca': 3051, 'pattern': 3052, 'difference': 3053, 'argentina': 3054, 'jean': 3055, 'phelps': 3056, 'copper': 3057, 'provides': 3058, 'pharmacy': 3059, 'andrew': 3060, 'conclusion': 3061, 'funded': 3062, 'rockies': 3063, 'client': 3064, 'hide': 3065, 'commit': 3066, 'firefox': 3067, 'browser': 3068, 'alternative': 3069, 'rough': 3070, 'master': 3071, 'bloomberg': 3072, 'shop': 3073, 'dr': 3074, 'eric': 3075, 'lady': 3076, 'reporter': 3077, 'joined': 3078, 'advocate': 3079, 'maintain': 3080, 'rite': 3081, 'hill': 3082, 'immediately': 3083, 'golden': 3084, 'vision': 3085, 'ray': 3086, 'brent': 3087, 'plea': 3088, 'plead': 3089, 'walker': 3090, 'pro': 3091, 'usatoday': 3092, 'promote': 3093, 'idg': 3094, 'upstart': 3095, 'computerworld': 3096, 'spy': 3097, 'shape': 3098, 'football': 3099, 'bat': 3100, 'wrap': 3101, 'oust': 3102, 'stayed': 3103, 'april': 3104, 'subject': 3105, 'monopoly': 3106, 'firing': 3107, 'sometimes': 3108, 'shook': 3109, 'perfect': 3110, 'true': 3111, 'multiple': 3112, 'parliament': 3113, 'dominated': 3114, 'hunt': 3115, 'article': 3116, 'reading': 3117, 'looked': 3118, 'pull': 3119, 'gaining': 3120, 'usually': 3121, 'swift': 3122, 'indefinitely': 3123, 'seventh': 3124, 'longest': 3125, 'plagued': 3126, 'cancel': 3127, 'crashed': 3128, 'barrier': 3129, 'philippine': 3130, 'receive': 3131, 'resolve': 3132, 'career': 3133, 'grabbed': 3134, 'pack': 3135, 'blame': 3136, 'host': 3137, 'hoped': 3138, 'myanmar': 3139, 'copy': 3140, 'fighting': 3141, 'sparked': 3142, 'fully': 3143, 'narrowly': 3144, 'discovery': 3145, 'licensing': 3146, 'rushing': 3147, 'coach': 3148, 'tomorrow': 3149, 'importance': 3150, 'phishing': 3151, 'baseball': 3152, 'meat': 3153, 'canceled': 3154, 'slowly': 3155, 'homer': 3156, 'cruised': 3157, 'leave': 3158, 'wash': 3159, 'nascar': 3160, 'carolina': 3161, 'preview': 3162, 'treo': 3163, 'smartphone': 3164, 'palmone': 3165, 'carry': 3166, 'battery': 3167, 'include': 3168, 'capability': 3169, 'capture': 3170, 'lenovo': 3171, 'household': 3172, 'walk': 3173, 'henry': 3174, 'believed': 3175, 'walked': 3176, 'intervention': 3177, 'remainder': 3178, 'elderly': 3179, 'ill': 3180, 'returned': 3181, 'stunt': 3182, 'horse': 3183, 'marking': 3184, 'mount': 3185, 'proved': 3186, 'victim': 3187, 'method': 3188, 'establish': 3189, 'transfer': 3190, 'reward': 3191, 'accepting': 3192, 'rout': 3193, 'finished': 3194, 'bottom': 3195, 'description': 3196, 'urban': 3197, 'premier': 3198, 'suffering': 3199, 'layer': 3200, 'dick': 3201, 'peter': 3202, 'classic': 3203, 'admits': 3204, 'lying': 3205, 'shut': 3206, 'extradition': 3207, 'lockheed': 3208, 'tumble': 3209, 'resignation': 3210, 'computing': 3211, 'contact': 3212, 'lens': 3213, 'irish': 3214, 'else': 3215, 'changing': 3216, 'comply': 3217, 'regulation': 3218, 'eli': 3219, 'lilly': 3220, 'obtaining': 3221, 'erp': 3222, 'roundup': 3223, 'creator': 3224, 'larry': 3225, 'switch': 3226, 'activated': 3227, 'playing': 3228, 'williams': 3229, 'rail': 3230, 'herald': 3231, 'quoted': 3232, 'bright': 3233, 'defending': 3234, 'incoming': 3235, 'malicious': 3236, 'bambang': 3237, 'unusual': 3238, 'taste': 3239, 'artery': 3240, 'jr': 3241, 'tip': 3242, 'worldcom': 3243, 'arkansas': 3244, 'terrorist': 3245, 'milwaukee': 3246, 'sen': 3247, 'criticized': 3248, 'stealing': 3249, 'gift': 3250, 'enter': 3251, 'africa': 3252, 'sooner': 3253, 'blood': 3254, 'doctor': 3255, 'outright': 3256, 'die': 3257, 'crash': 3258, 'fleet': 3259, 'bold': 3260, 'scottish': 3261, 'martha': 3262, 'sentence': 3263, 'absence': 3264, 'outsourcing': 3265, 'abbott': 3266, 'jamaica': 3267, 'powerhouse': 3268, 'scientist': 3269, 'pole': 3270, 'hurdle': 3271, 'mississippi': 3272, 'relation': 3273, 'twin': 3274, 'kevin': 3275, 'handheld': 3276, 'example': 3277, 'boy': 3278, 'procedure': 3279, 'sit': 3280, 'initiative': 3281, 'interested': 3282, 'complex': 3283, 'jailed': 3284, 'guide': 3285, 'refused': 3286, 'tape': 3287, 'osama': 3288, 'bin': 3289, 'stuck': 3290, 'membership': 3291, 'governing': 3292, 'register': 3293, 'wilson': 3294, 'landing': 3295, 'novell': 3296, 'voip': 3297, 'jackson': 3298, 'great': 3299, 'escape': 3300, 'destroyed': 3301, 'impressive': 3302, 'greenhouse': 3303, 'academic': 3304, 'adopt': 3305, 'emission': 3306, 'warming': 3307, 'graphic': 3308, 'lab': 3309, 'compact': 3310, 'earth': 3311, 'nec': 3312, 'achieved': 3313, 'onto': 3314, 'fact': 3315, 'thin': 3316, 'neck': 3317, 'brain': 3318, 'walter': 3319, 'marshall': 3320, 'billionaire': 3321, 'leap': 3322, 'counterpart': 3323, 'allowed': 3324, 'allowing': 3325, 'confirm': 3326, 'speak': 3327, 'lure': 3328, 'ton': 3329, 'succession': 3330, 'transition': 3331, 'atop': 3332, 'dominican': 3333, 'republic': 3334, 'leg': 3335, 'knocked': 3336, 'surrender': 3337, 'blog': 3338, 'programme': 3339, 'czech': 3340, 'suggestion': 3341, 'sloppy': 3342, 'league': 3343, 'dog': 3344, 'israel': 3345, 'gaza': 3346, 'strip': 3347, 'launching': 3348, 'candidate': 3349, 'oh': 3350, 'reveal': 3351, 'origin': 3352, 'charging': 3353, 'captive': 3354, 'connecticut': 3355, 'aggressive': 3356, 'sideline': 3357, 'landmark': 3358, 'tactic': 3359, 'msn': 3360, 'itunes': 3361, 'lloyd': 3362, 'conviction': 3363, 'kazaa': 3364, 'tony': 3365, 'blair': 3366, 'hate': 3367, 'er': 3368, 'abu': 3369, 'resume': 3370, 'cat': 3371, 'switzerland': 3372, 'narrow': 3373, 'praise': 3374, 'hostage': 3375, 'maverick': 3376, 'overhaul': 3377, 'hp': 3378, 'printer': 3379, 'deciding': 3380, 'vietnam': 3381, 'hopeful': 3382, 'penn': 3383, 'education': 3384, 'skill': 3385, 'fame': 3386, 'troop': 3387, 'angry': 3388, 'critic': 3389, 'felt': 3390, 'packer': 3391, 'marketplace': 3392, 'sound': 3393, 'inched': 3394, 'harrington': 3395, 'shortfall': 3396, 'permanent': 3397, 'cheer': 3398, 'rick': 3399, 'responsibility': 3400, 'brigham': 3401, 'died': 3402, 'flag': 3403, 'sponsored': 3404, 'nevada': 3405, 'homeland': 3406, 'suspect': 3407, 'palestinian': 3408, 'israeli': 3409, 'arena': 3410, 'content': 3411, 'lock': 3412, 'info': 3413, 'restore': 3414, 'palm': 3415, 'beach': 3416, 'protocol': 3417, 'postponed': 3418, 'showdown': 3419, 'cowboy': 3420, 'hat': 3421, 'helicopter': 3422, 'dig': 3423, 'beneath': 3424, 'leak': 3425, 'crystal': 3426, 'quite': 3427, 'fbi': 3428, 'raid': 3429, 'seized': 3430, 'meter': 3431, 'mother': 3432, 'delhi': 3433, 'complaint': 3434, 'trail': 3435, 'briton': 3436, 'drama': 3437, 'joe': 3438, 'unveil': 3439, 'finding': 3440, 'explanation': 3441, 'pocket': 3442, 'belgian': 3443, 'rain': 3444, 'battled': 3445, 'sight': 3446, 'upgrading': 3447, 'tied': 3448, 'mike': 3449, 'pas': 3450, 'height': 3451, 'necessary': 3452, 'hitting': 3453, 'village': 3454, 'stockholm': 3455, 'iron': 3456, 'chile': 3457, 'smith': 3458, 'qatar': 3459, 'kuwait': 3460, 'desktop': 3461, 'privacy': 3462, 'chat': 3463, 'learning': 3464, 'respect': 3465, 'animal': 3466, 'inspection': 3467, 'sample': 3468, 'plot': 3469, 'attend': 3470, 'delegation': 3471, 'exhibition': 3472, 'down': 3473, 'nationwide': 3474, 'cuba': 3475, 'picking': 3476, 'color': 3477, 'lane': 3478, 'ryder': 3479, 'freed': 3480, 'investigator': 3481, 'discovered': 3482, 'suffer': 3483, 'stopped': 3484, 'jointly': 3485, 'franchise': 3486, 'immigrant': 3487, 'ed': 3488, 'ntt': 3489, 'docomo': 3490, 'mode': 3491, 'shanghai': 3492, 'killer': 3493, 'frustration': 3494, 'wound': 3495, 'journalist': 3496, 'disappointment': 3497, 'ii': 3498, 'happy': 3499, 'embargo': 3500, 'relatively': 3501, 'carlos': 3502, 'cuban': 3503, 'dealt': 3504, 'triple': 3505, 'shocked': 3506, 'imposed': 3507, 'spyware': 3508, 'buck': 3509, 'jail': 3510, 'icon': 3511, 'towards': 3512, 'milestone': 3513, 'responded': 3514, 'solve': 3515, 'appearance': 3516, 'dawn': 3517, 'resolved': 3518, 'derek': 3519, 'pac': 3520, 'recording': 3521, 'included': 3522, 'attacked': 3523, 'intelligence': 3524, 'declared': 3525, 'gang': 3526, 'undergo': 3527, 'staged': 3528, 'hughes': 3529, 'disabled': 3530, 'kim': 3531, 'repair': 3532, 'attention': 3533, 'suddenly': 3534, 'confirms': 3535, 'finger': 3536, 'quietly': 3537, 'neighbor': 3538, 'compound': 3539, 'landslide': 3540, 'entry': 3541, 'mccain': 3542, 'grade': 3543, 'delighted': 3544, 'ot': 3545, 'witness': 3546, 'immunity': 3547, 'antonio': 3548, 'match': 3549, 'tested': 3550, 'relative': 3551, 'colorado': 3552, 'transplant': 3553, 'simply': 3554, 'nintendo': 3555, 'santa': 3556, 'crowded': 3557, 'pirated': 3558, 'nl': 3559, 'turkey': 3560, 'galaxy': 3561, 'covered': 3562, 'titan': 3563, 'im': 3564, 'moderate': 3565, 'creation': 3566, 'ron': 3567, 'christian': 3568, 'monitor': 3569, 'finishing': 3570, 'historic': 3571, 'crunch': 3572, 'freedom': 3573, 'collaboration': 3574, 'array': 3575, 'hammer': 3576, 'virginia': 3577, 'basic': 3578, 'eve': 3579, 'congressional': 3580, 'jose': 3581, 'lion': 3582, 'rover': 3583, 'redskin': 3584, 'father': 3585, 'jordan': 3586, 'neighborhood': 3587, 'baghdad': 3588, 'tourism': 3589, 'passport': 3590, 'revenge': 3591, 'code': 3592, 'thumb': 3593, 'threw': 3594, 'fined': 3595, 'rbi': 3596, 'saint': 3597, 'explosive': 3598, 'belief': 3599, 'custom': 3600, 'promoting': 3601, 'score': 3602, 'physical': 3603, 'integrated': 3604, 'chosen': 3605, 'delivers': 3606, 'artist': 3607, 'ranked': 3608, 'wish': 3609, 'stone': 3610, 'austin': 3611, 'unsolicited': 3612, 'jungle': 3613, 'quest': 3614, 'rolled': 3615, 'columbia': 3616, 'spectacular': 3617, 'ocean': 3618, 'mason': 3619, 'controversy': 3620, 'lord': 3621, 'surfer': 3622, 'password': 3623, 'ken': 3624, 'iowa': 3625, 'sacked': 3626, 'anderson': 3627, 'bundle': 3628, 'glory': 3629, 'orleans': 3630, 'landed': 3631, 'charger': 3632, 'unveiling': 3633, 'audience': 3634, 'plotting': 3635, 'gather': 3636, 'vienna': 3637, 'described': 3638, 'samsung': 3639, 'liverpool': 3640, 'convert': 3641, 'sap': 3642, 'capable': 3643, 'mar': 3644, 'stuttgart': 3645, 'entrepreneur': 3646, 'islamabad': 3647, 'silence': 3648, 'arrival': 3649, 'kenneth': 3650, 'kentucky': 3651, 'ati': 3652, 'birmingham': 3653, 'fault': 3654, 'spur': 3655, 'explosion': 3656, 'bag': 3657, 'collision': 3658, 'tank': 3659, 'twist': 3660, 'suspected': 3661, 'nashville': 3662, 'dual': 3663, 'criticism': 3664, 'mortar': 3665, 'elite': 3666, 'lake': 3667, 'enemy': 3668, 'monster': 3669, 'ftc': 3670, 'zurich': 3671, 'pledged': 3672, 'dc': 3673, 'toughest': 3674, 'releasing': 3675, 'dominance': 3676, 'perez': 3677, 'reader': 3678, 'read': 3679, 'print': 3680, 'cardinal': 3681, 'ancient': 3682, 'heading': 3683, 'error': 3684, 'roger': 3685, 'finland': 3686, 'sweden': 3687, 'sox': 3688, 'fenway': 3689, 'standing': 3690, 'designer': 3691, 'craft': 3692, 'ben': 3693, 'done': 3694, 'arrest': 3695, 'wale': 3696, 'beaten': 3697, 'grip': 3698, 'ta': 3699, 'magnitude': 3700, 'pop': 3701, 'carrying': 3702, 'bob': 3703, 'dealing': 3704, 'sweep': 3705, 'pending': 3706, 'espn': 3707, 'pulling': 3708, 'ryan': 3709, 'traveling': 3710, 'pitch': 3711, 'complicated': 3712, 'punish': 3713, 'cease': 3714, 'notebook': 3715, 'politician': 3716, 'popularity': 3717, 'recorder': 3718, 'luis': 3719, 'earn': 3720, 'spam': 3721, 'incident': 3722, 'smart': 3723, 'entrance': 3724, 'yes': 3725, 'scrap': 3726, 'politics': 3727, 'negotiating': 3728, 'par': 3729, 'supporter': 3730, 'napster': 3731, 'bbc': 3732, 'villager': 3733, 'hosting': 3734, 'pursuit': 3735, 'troy': 3736, 'wire': 3737, 'bomb': 3738, 'bombed': 3739, 'abc': 3740, 'regular': 3741, 'verdana': 3742, 'm': 3743, 'sans': 3744, 'serif': 3745, 'arial': 3746, 'helvetica': 3747, 'font': 3748, 'dna': 3749, 'genetic': 3750, 'identification': 3751, 'carbon': 3752, 'dioxide': 3753, 'burning': 3754, 'unknown': 3755, 'trapped': 3756, 'certificate': 3757, 'interesting': 3758, 'i': 3759, 'volcano': 3760, 'trojan': 3761, 'text': 3762, 'purportedly': 3763, 'virus': 3764, 'adoption': 3765, 'astronomer': 3766, 'photograph': 3767, 'solar': 3768, 'infoworld': 3769, 'longhorn': 3770, 'relay': 3771, 'nasa': 3772, 'demonstration': 3773, 'mission': 3774, 'drawn': 3775, 'specie': 3776, 'experience': 3777, 'pda': 3778, 'seoul': 3779, 'easily': 3780, 'eclipse': 3781, 'embrace': 3782, 'activist': 3783, 'surface': 3784, 'contain': 3785, 'democrat': 3786, 'fish': 3787, 'console': 3788, 'techweb': 3789, 'pentium': 3790, 'extreme': 3791, 'il': 3792, 'spacecraft': 3793, 'particle': 3794, 'newsfactor': 3795, 'arctic': 3796, 'clue': 3797, 'specifically': 3798, 'shuttle': 3799, 'gb': 3800, 'dictionary': 3801, 'spaceshipone': 3802, 'itanium': 3803, 'connect': 3804, 'tune': 3805, 'sp': 3806, 'falcon': 3807, 'wildlife': 3808, 'handing': 3809, 'firewall': 3810, 'remote': 3811, 'trace': 3812, 'ie': 3813, 'moon': 3814, 'km': 3815, 'suite': 3816, 'hacker': 3817, 'blu': 3818, 'teenager': 3819, 'sasser': 3820, 'worm': 3821, 'rfid': 3822, 'baltimore': 3823, 'closest': 3824, 'saturn': 3825, 'captured': 3826, 'cassini': 3827, 'flew': 3828, 'sm': 3829, 'beta': 3830, 'fingerprint': 3831, 'vulnerability': 3832, 'aiming': 3833, 'specification': 3834, 'featuring': 3835, 'backer': 3836, 'soccer': 3837, 'beckham': 3838, 'router': 3839, 'swimming': 3840, 'sex': 3841, 'shoot': 3842, 'assassination': 3843, 'mouse': 3844, 'muscle': 3845, 'exploit': 3846, 'jpeg': 3847, 'intended': 3848, 'lycos': 3849, 'screensaver': 3850, 'playstation': 3851, 'killing': 3852, 'ear': 3853, 'band': 3854, 'venus': 3855, 'dubbed': 3856, 'targeted': 3857, 'arrested': 3858, 'dominating': 3859, 'fujitsu': 3860, 'epidemic': 3861, 'scramjet': 3862, 'mph': 3863, 'usage': 3864, 'behavior': 3865, 'max': 3866, 'nice': 3867, 'bloody': 3868, 'genesis': 3869, 'capsule': 3870, 'utah': 3871, 'pnet': 3872, 'thailand': 3873, 'citizen': 3874, 'strained': 3875, 'violent': 3876, 'punch': 3877, 'install': 3878, 'successfully': 3879, 'female': 3880, 'metre': 3881, 'elephant': 3882, 'desert': 3883, 'dave': 3884, 'elected': 3885, 'cyber': 3886, 'hall': 3887, 'sender': 3888, 'authentication': 3889, 'audio': 3890, 'palmsource': 3891, 'ansari': 3892, 'starter': 3893, 'scored': 3894, 'liberal': 3895, 'bali': 3896, 'duel': 3897, 'washingtonpost': 3898, 'helen': 3899, 'campus': 3900, 'maccentral': 3901, 'o': 3902, 'chris': 3903, 'gadget': 3904, 'email': 3905, 'fit': 3906, 'astronaut': 3907, 'tiny': 3908, 'psp': 3909, 'availability': 3910, 'prisoner': 3911, 'd': 3912, 'glacier': 3913, 'qtype': 3914, 'sym': 3915, 'infotype': 3916, 'qcat': 3917, 'arrive': 3918, 'nvidia': 3919, 'bus': 3920, 'surveillance': 3921, 'pirate': 3922, 'bug': 3923, 'chilean': 3924, 'demonstrated': 3925, 'function': 3926, 'understand': 3927, 'experiment': 3928, 'allen': 3929, 'telescope': 3930, 'theory': 3931, 'dy': 3932, 'legend': 3933, 'sutton': 3934, 'mysterious': 3935, 'imac': 3936, 'expo': 3937, 'mary': 3938, 'malware': 3939, 'resign': 3940, 'sean': 3941, 'keefe': 3942, 'scott': 3943, 'battleground': 3944, 'tightened': 3945, 'male': 3946, 'ziff': 3947, 'davis': 3948, 'secretly': 3949, 'stretch': 3950, 'quake': 3951, 'mystery': 3952, 'earthquake': 3953, 'substance': 3954, 'orange': 3955, 'soviet': 3956, 'completing': 3957, 'louisville': 3958, 'evolution': 3959, 'grid': 3960, 'drought': 3961, 'dust': 3962, 'bowl': 3963, 'duke': 3964, 'orbit': 3965, 'packed': 3966, 'sri': 3967, 'lanka': 3968, 'submarine': 3969, 'circulating': 3970, 'curse': 3971, 'blunkett': 3972, 'supercomputing': 3973, 'ram': 3974, 'bounty': 3975, 'busy': 3976, 'determined': 3977, 'eventually': 3978, 'skull': 3979, 'xbox': 3980, 'knee': 3981, 'robot': 3982, 'attacker': 3983, 'soyuz': 3984, 'andy': 3985, 'outbreak': 3986, 'album': 3987, 'singer': 3988, 'mozilla': 3989, 'clie': 3990, 'unix': 3991, 'simon': 3992, 'invasion': 3993, 'austria': 3994, 'ambition': 3995, 'adware': 3996, 'unique': 3997, 'native': 3998, 'useful': 3999, 'cheney': 4000, 'finale': 4001, 'athlete': 4002, 'aboard': 4003, 'solaris': 4004, 'atomic': 4005, 'crush': 4006, 'installed': 4007, 'opposed': 4008, 'stripped': 4009, 'button': 4010, 'roman': 4011, 'nuke': 4012, 'jboss': 4013, 'pci': 4014, 'comet': 4015, 'fred': 4016, 'truly': 4017, 'workplace': 4018, 'siliconvalley': 4019, 'patriot': 4020, 'unmanned': 4021, 'hubble': 4022, 'tiger': 4023, 'birth': 4024, 'congo': 4025, 'netscape': 4026, 'hero': 4027, 'ape': 4028, 'app': 4029, 'element': 4030, 'missing': 4031, 'knowledge': 4032, 'clock': 4033, 'flown': 4034, 'lunar': 4035, 'professor': 4036, 'gartner': 4037, 'meaning': 4038, 'costello': 4039, 'anniversary': 4040, 'church': 4041, 'alexander': 4042, 'moss': 4043, 'jason': 4044, 'virtual': 4045, 'murder': 4046, 'whale': 4047, 'ceremony': 4048, 'amateur': 4049, 'particularly': 4050, 'maybe': 4051, 'lap': 4052, 'spirit': 4053, 'overture': 4054, 'tumor': 4055, 'mate': 4056, 'trick': 4057, 'nearby': 4058, 'orlando': 4059, 'architecture': 4060, 'sting': 4061, 'skype': 4062, 'shirt': 4063, 'tone': 4064, 'paypal': 4065, 'rivera': 4066, 'vast': 4067, 'feed': 4068, 'dry': 4069, 'concluded': 4070, 'glitch': 4071, 'celebrated': 4072, 'halfway': 4073, 'assault': 4074, 'dismissed': 4075, 'contention': 4076, 'sven': 4077, 'formally': 4078, 'kit': 4079, 'magic': 4080, 'nfc': 4081, 'symbian': 4082, 'manned': 4083, 'kyoto': 4084, 'arsenal': 4085, 'dead': 4086, 'movement': 4087, 'tackle': 4088, 'seeker': 4089, 'blew': 4090, 'dictator': 4091, 'burma': 4092, 'fernando': 4093, 'doping': 4094, 'summary': 4095, 'doc': 4096, 'talking': 4097, 'jihad': 4098, 'islamic': 4099, 'gay': 4100, 'alive': 4101, 'burn': 4102, 'couple': 4103, 'devil': 4104, 'beagle': 4105, 'colin': 4106, 'blade': 4107, 'emotional': 4108, 'seed': 4109, 'rex': 4110, 'injures': 4111, 'peru': 4112, 'ceasefire': 4113, 'southwestern': 4114, 'patrol': 4115, 'identify': 4116, 'unlike': 4117, 'sep': 4118, 'phoenix': 4119, 'thrown': 4120, 'ghz': 4121, 'ferrari': 4122, 'medal': 4123, 'receiver': 4124, 'gmail': 4125, 'matt': 4126, 'kerr': 4127, 'pakistani': 4128, 'happen': 4129, 'pittsburgh': 4130, 'miracle': 4131, 'bone': 4132, 'comeback': 4133, 'civilian': 4134, 'dome': 4135, 'patrick': 4136, 'locust': 4137, 'appealed': 4138, 'scan': 4139, 'randy': 4140, 'rescuer': 4141, 'lsu': 4142, 'northwestern': 4143, 'ambassador': 4144, 'regret': 4145, 'eddie': 4146, 'jonathan': 4147, 'twenty': 4148, 'dan': 4149, 'settler': 4150, 'nfl': 4151, 'retires': 4152, 'kent': 4153, 'identified': 4154, 'aaron': 4155, 'yang': 4156, 'taipei': 4157, 'tom': 4158, 'verdict': 4159, 'elena': 4160, 'knew': 4161, 'bryant': 4162, 'nba': 4163, 'kobe': 4164, 'lone': 4165, 'convicted': 4166, 'piston': 4167, 'exploded': 4168, 'shawn': 4169, 'tournament': 4170, 'wearing': 4171, 'ethnic': 4172, 'rom': 4173, 'evacuation': 4174, 'recruit': 4175, 'wisconsin': 4176, 'holy': 4177, 'hitter': 4178, 'banned': 4179, 'striker': 4180, 'marathon': 4181, 'teacher': 4182, 'gesture': 4183, 'condemned': 4184, 'matthew': 4185, 'polish': 4186, 'luke': 4187, 'bronze': 4188, 'minor': 4189, 'baby': 4190, 'chair': 4191, 'removed': 4192, 'chad': 4193, 'tourist': 4194, 'tennis': 4195, 'championship': 4196, 'tent': 4197, 'cup': 4198, 'tropical': 4199, 'seriously': 4200, 'dolphin': 4201, 'nick': 4202, 'shooting': 4203, 'remarkable': 4204, 'demanding': 4205, 'denver': 4206, 'defender': 4207, 'insists': 4208, 'frustrated': 4209, 'madrid': 4210, 'buried': 4211, 'undergoing': 4212, 'danny': 4213, 'qualifier': 4214, 'gunman': 4215, 'upset': 4216, 'opener': 4217, 'manchester': 4218, 'missile': 4219, 'anthony': 4220, 'cyprus': 4221, 'extraordinary': 4222, 'madison': 4223, 'presented': 4224, 'tribute': 4225, 'maurice': 4226, 'tsunami': 4227, 'naval': 4228, 'yankee': 4229, 'yard': 4230, 'venue': 4231, 'shoulder': 4232, 'alex': 4233, 'militant': 4234, 'muslim': 4235, 'refuse': 4236, 'basketball': 4237, 'wood': 4238, 'greg': 4239, 'ibrahim': 4240, 'manning': 4241, 'curt': 4242, 'unity': 4243, 'stormed': 4244, 'wizard': 4245, 'oklahoma': 4246, 'steelers': 4247, 'sharapova': 4248, 'slugger': 4249, 'ortiz': 4250, 'mvp': 4251, 'postseason': 4252, 'uefa': 4253, 'hockey': 4254, 'safin': 4255, 'marat': 4256, 'tying': 4257, 'artest': 4258, 'neal': 4259, 'peaceful': 4260, 'rookie': 4261, 'rugby': 4262, 'nhl': 4263, 'rafael': 4264, 'benitez': 4265, 'barry': 4266, 'notre': 4267, 'dame': 4268, 'brawl': 4269, 'brian': 4270, 'vijay': 4271, 'squad': 4272, 'pga': 4273, 'playoff': 4274, 'ncaa': 4275, 'hamm': 4276, 'captain': 4277, 'midfielder': 4278, 'pitching': 4279, 'pitcher': 4280, 'schilling': 4281, 'oriole': 4282, 'baseman': 4283, 'thierry': 4284, 'premiership': 4285, 'villa': 4286, 'derby': 4287, 'lima': 4288, 'shutout': 4289, 'dodger': 4290, 'pitched': 4291, 'ichiro': 4292, 'mariner': 4293, 'ranger': 4294, 'chelsea': 4295, 'everton': 4296, 'juan': 4297, 'mcnair': 4298, 'jacksonville': 4299, 'quarterback': 4300, 'ganguly': 4301, 'gymnastics': 4302, 'clinch': 4303, 'berth': 4304, 'touchdown': 4305, 'teammate': 4306, 'paula': 4307, 'seeded': 4308, 'gibbs': 4309, 'seahawks': 4310, 'undefeated': 4311, 'guillen': 4312, 'catcher': 4313, 'rodriguez': 4314, 'pedro': 4315, 'martinez': 4316, 'mets': 4317, 'bcs': 4318, 'meyer': 4319, 'formula': 4320, 'ernie': 4321, 'memphis': 4322, 'portland': 4323, 'raptor': 4324, 'jacques': 4325, 'radcliffe': 4326, 'roadside': 4327, 'hamilton': 4328, 'pennington': 4329, 'tampa': 4330, 'athletics': 4331, 'inning': 4332, 'racist': 4333, 'aussie': 4334, 'newman': 4335, 'ticker': 4336, 'clipper': 4337, 'pacer': 4338, 'scot': 4339, 'marlin': 4340, 'bloomfield': 4341, 'montgomerie': 4342, 'broadhurst': 4343, 'wimbledon': 4344, 'maria': 4345, 'capped': 4346, 'serena': 4347, 'wta': 4348, 'wenger': 4349, 'arsene': 4350, 'buccaneer': 4351, 'jay': 4352, 'offense': 4353, 'hornet': 4354, 'wicket': 4355, 'diamondback': 4356, 'yank': 4357, 'indie': 4358, 'ian': 4359, 'kyle': 4360, 'castro': 4361, 'agassi': 4362, 'lindsay': 4363, 'davenport': 4364, 'skipper': 4365, 'cricket': 4366, 'english': 4367, 'fa': 4368, 'triumph': 4369, 'qualifying': 4370, 'roddick': 4371, 'andre': 4372, 'bronco': 4373, 'defensive': 4374, 'manny': 4375, 'ramirez': 4376, 'brave': 4377, 'outfielder': 4378, 'sprinter': 4379, 'erupted': 4380, 'raven': 4381, 'prix': 4382, 'linebacker': 4383, 'newcastle': 4384, 'viking': 4385, 'hewitt': 4386, 'quarterfinal': 4387, 'lleyton': 4388, 'duo': 4389, 'adrian': 4390, 'stadium': 4391, 'souness': 4392, 'graeme': 4393, 'breeder': 4394, 'colt': 4395, 'peyton': 4396, 'trafford': 4397, 'knicks': 4398, 'gator': 4399, 'pounded': 4400, 'angel': 4401, 'scoring': 4402, 'anaheim': 4403, 'pennant': 4404, 'henman': 4405, 'federer': 4406, 'jerry': 4407, 'matchup': 4408, 'panther': 4409, 'rooney': 4410, 'wayne': 4411, 'preseason': 4412, 'semifinal': 4413, 'coaching': 4414, 'novak': 4415, 'blackburn': 4416, 'smile': 4417, 'astros': 4418, 'clemens': 4419, 'mauresmo': 4420, 'amelie': 4421, 'favourite': 4422, 'cornerback': 4423, 'bowler': 4424, 'unbeaten': 4425, 'bode': 4426, 'cycling': 4427, 'defeated': 4428, 'fifa': 4429, 'golfer': 4430, 'funeral': 4431, 'plate': 4432, 'lara': 4433, 'zook': 4434, 'panama': 4435, 'racism': 4436, 'sprained': 4437, 'brett': 4438, 'mourning': 4439, 'jamal': 4440, 'boxing': 4441, 'auburn': 4442, 'serie': 4443, 'seize': 4444, 'brandon': 4445, 'gerrard': 4446, 'homered': 4447, 'timberwolves': 4448, 'charlton': 4449, 'tear': 4450, 'atp': 4451, 'steroid': 4452, 'tailback': 4453, 'trophy': 4454, 'lakers': 4455, 'champ': 4456, 'byu': 4457, 'torn': 4458, 'typhoon': 4459, 'schumacher': 4460, 'gatlin': 4461, 'greene': 4462, 'ukraine': 4463, 'carter': 4464, 'defeating': 4465, 'loeb': 4466, 'favre': 4467, 'icc': 4468, 'robinson': 4469, 'glenn': 4470, 'medalist': 4471, 'haas': 4472, 'kurt': 4473, 'busch': 4474, 'swim': 4475, 'inaugural': 4476, 'romanian': 4477, 'mutu': 4478, 'ricky': 4479, 'zimbabwe': 4480, 'marriage': 4481, 'valencia': 4482, 'eriksson': 4483, 'goran': 4484, 'maiden': 4485, 'hamstring': 4486, 'cocaine': 4487, 'ioc': 4488, 'cub': 4489, 'lyon': 4490, 'kidnapped': 4491, 'batsman': 4492, 'referee': 4493, 'thatcher': 4494, 'palace': 4495, 'rushed': 4496, 'ferguson': 4497, 'bayern': 4498, 'bundesliga': 4499, 'ronaldo': 4500, 'cavalier': 4501, 'schalke': 4502, 'hander': 4503, 'portsmouth': 4504, 'ankle': 4505, 'halftime': 4506, 'phillies': 4507, 'paralympics': 4508, 'lpga': 4509, 'purdue': 4510, 'bryan': 4511, 'priest': 4512, 'riot': 4513, 'argentine': 4514, 'payton': 4515, 'sorenstam': 4516, 'tendulkar': 4517, 'supersonics': 4518, 'dunhill': 4519, 'cabrera': 4520, 'garcia': 4521, 'langer': 4522, 'rape': 4523, 'bobcat': 4524, 'stormy': 4525, 'willingham': 4526, 'spurrier': 4527, 'powell': 4528, 'communist': 4529, 'theo': 4530, 'injuring': 4531, 'displaced': 4532, 'doubtful': 4533, 'frenchman': 4534, 'freshman': 4535, 'possession': 4536, 'bobby': 4537, 'bangladesh': 4538, 'kremlin': 4539, 'mourinho': 4540, 'jenkins': 4541, 'northeastern': 4542, 'kiev': 4543, 'barbados': 4544, 'embassy': 4545, 'armstrong': 4546, 'haiti': 4547, 'coordinator': 4548, 'dhaka': 4549, 'tense': 4550, 'omar': 4551, 'colombia': 4552, 'santiago': 4553, 'egypt': 4554, 'parcells': 4555, 'wounded': 4556, 'catholic': 4557, 'ali': 4558, 'egyptian': 4559, 'libya': 4560, 'anger': 4561, 'occupied': 4562, 'videotape': 4563, 'salvador': 4564, 'afghan': 4565, 'regime': 4566, 'kidnapper': 4567, 'abducted': 4568, 'chen': 4569, 'haitian': 4570, 'protester': 4571, 'kashmir': 4572, 'tehran': 4573, 'guinea': 4574, 'afghanistan': 4575, 'kabul': 4576, 'envoy': 4577, 'tribal': 4578, 'sadr': 4579, 'jewish': 4580, 'shui': 4581, 'bian': 4582, 'musharraf': 4583, 'pervez': 4584, 'hamid': 4585, 'karzai': 4586, 'separatist': 4587, 'dialogue': 4588, 'insurgent': 4589, 'bombing': 4590, 'fallujah': 4591, 'kofi': 4592, 'allawi': 4593, 'cambodia': 4594, 'norodom': 4595, 'beheaded': 4596, 'kidnapping': 4597, 'grenade': 4598, 'qaeda': 4599, 'sudanese': 4600, 'darfur': 4601, 'refugee': 4602, 'disarm': 4603, 'sudan': 4604, 'militiaman': 4605, 'diplomatic': 4606, 'zarqawi': 4607, 'musab': 4608, 'rumsfeld': 4609, 'nato': 4610, 'enrichment': 4611, 'mohammad': 4612, 'qaida': 4613, 'warlord': 4614, 'diverted': 4615, 'arafat': 4616, 'ramallah': 4617, 'yasser': 4618, 'democracy': 4619, 'bigley': 4620, 'taliban': 4621, 'inmate': 4622, 'guantanamo': 4623, 'sharon': 4624, 'jerusalem': 4625, 'ariel': 4626, 'parliamentary': 4627, 'hamas': 4628, 'mosque': 4629, 'sunni': 4630, 'stronghold': 4631, 'suu': 4632, 'islamist': 4633, 'jazeera': 4634, 'orthodox': 4635, 'convoy': 4636, 'delegate': 4637, 'cleric': 4638, 'shiite': 4639, 'muqtada': 4640, 'uprising': 4641, 'rwanda': 4642, 'policeman': 4643, 'beslan': 4644, 'captor': 4645, 'margaret': 4646, 'hassan': 4647, 'khartoum': 4648, 'humanitarian': 4649, 'elect': 4650, 'detainee': 4651, 'annan': 4652, 'slum': 4653, 'moqtada': 4654, 'kathmandu': 4655, 'maoist': 4656, 'samarra': 4657, 'imam': 4658, 'massacre': 4659, 'gunfire': 4660, 'aristide': 4661, 'ghraib': 4662, 'lebanese': 4663, 'genocide': 4664, 'pullout': 4665, 'peacekeeper': 4666, 'wounding': 4667, 'nepal': 4668, 'pyongyang': 4669, 'barghouti': 4670, 'sworn': 4671, 'chechnya': 4672, 'falluja': 4673, 'mosul': 4674, 'milosevic': 4675, 'yugoslav': 4676, 'slobodan': 4677, 'turkish': 4678, 'abuja': 4679, 'syrian': 4680, 'syria': 4681, 'bashir': 4682, 'bakar': 4683, 'extremist': 4684, 'guerrilla': 4685, 'eta': 4686, 'detained': 4687, 'serb': 4688, 'murdered': 4689, 'cairo': 4690, 'citizenship': 4691, 'berlusconi': 4692, 'custody': 4693, 'insurgency': 4694, 'pope': 4695, 'asylum': 4696, 'detention': 4697, 'gogh': 4698, 'chechen': 4699, 'fled': 4700, 'beirut': 4701, 'lebanon': 4702, 'damascus': 4703, 'iranian': 4704, 'deadliest': 4705, 'pitcairn': 4706, 'anwar': 4707, 'straw': 4708, 'serbian': 4709, 'shaukat': 4710, 'aziz': 4711, 'nepalese': 4712, 'likud': 4713, 'pinochet': 4714}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.news_vocab._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Business': 0, 'Sci/Tech': 1, 'Sports': 2, 'World': 3}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.category_vocab._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset._max_seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.002992868423461914,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "training routine",
       "rate": null,
       "total": 100,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "760d35a6521a4f4c9be85dec98a01216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0029914379119873047,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "split=train",
       "rate": null,
       "total": 181,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a922a1581504548b97c5142549022f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0039899349212646484,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "split=val",
       "rate": null,
       "total": 38,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611e00ed4e304715b58b16d9fa9985ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "    \n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.5, patience=1)\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm(desc='training routine', total=args.num_epochs, position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm(desc='split=train', total=dataset.get_num_batches(args.batch_size), position=1, leave=True)\n",
    "\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm(desc='split=val', total=dataset.get_num_batches(args.batch_size), position=1, leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # Iterate over training dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # the training routine is these 5 steps:\n",
    "\n",
    "            # --------------------------------------\n",
    "            # step 1. zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # step 2. compute the output\n",
    "            y_pred = classifier(batch_dict['x_data']) # (batch, seq_len) -> (batch, num_classes)\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # step 4. use loss to produce gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # step 5. use optimizer to take gradient step\n",
    "            optimizer.step()\n",
    "            # -----------------------------------------\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # update bar\n",
    "            train_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                                  epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # Iterate over val dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "            # compute the output\n",
    "            y_pred =  classifier(batch_dict['x_data'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                            epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(args=args, model=classifier,\n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = train_state['train_acc']\n",
    "val_acc = train_state['val_acc']\n",
    "loss = train_state['train_loss']\n",
    "val_loss = train_state['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()   # clear figure\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the loss & accuracy on the test set using the best available model\n",
    "\n",
    "classifier.load_state_dict(torch.load(train_state['model_filename'])) # load the best model\n",
    "\n",
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "y_pred_list = []         # store predicted values for confusion matrix\n",
    "y_category_list = []  # ground truth value\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_pred =  classifier(batch_dict['x_data'])\n",
    "    \n",
    "    # store predicted values and ground truth values for calculating confusion matrix\n",
    "    y_pred_list.extend(y_pred.max(dim=1)[1].cpu().numpy())\n",
    "    y_category_list.extend(batch_dict['y_target'].cpu().numpy())\n",
    "    \n",
    "    # compute the loss\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # compute the accuracy\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test loss: {};\".format(train_state['test_loss']))\n",
    "print(\"Test Accuracy: {}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = []\n",
    "for i in range(len(dataset._vectorizer.category_vocab)):\n",
    "    classes.append(dataset._vectorizer.category_vocab.lookup_index(i))\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "cm = confusion_matrix(y_category_list, y_pred_list)\n",
    "cm_df = pd.DataFrame(cm.T, index=classes, columns=classes)\n",
    "cm_df.index.name = 'Predicted'\n",
    "cm_df.columns.name = 'True'\n",
    "print(cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_category_list, y_pred_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the reviews\n",
    "def preprocess_text(text):\n",
    "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_category(title, classifier, vectorizer, max_length):\n",
    "    \"\"\"Predict a News category for a new title\n",
    "    \n",
    "    Args:\n",
    "        title (str): a raw title string\n",
    "        classifier (NewsClassifier): an instance of the trained classifier\n",
    "        vectorizer (NewsVectorizer): the corresponding vectorizer\n",
    "        max_length (int): the max sequence length\n",
    "            Note: CNNs are sensitive to the input data tensor size. \n",
    "                  This ensures to keep it the same size as the training data\n",
    "    \"\"\"\n",
    "    title = preprocess_text(title)\n",
    "    vectorized_title = \\\n",
    "        torch.tensor(vectorizer.vectorize(title, vector_length=max_length))\n",
    "    result = classifier(vectorized_title.unsqueeze(0))\n",
    "    probability_values, indices = result.max(dim=1)\n",
    "    predicted_category = vectorizer.category_vocab.lookup_index(indices.item())\n",
    "\n",
    "    return {'category': predicted_category, \n",
    "            'probability': probability_values.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples():\n",
    "    samples = {}\n",
    "    for cat in dataset.val_df.category.unique():\n",
    "        samples[cat] = dataset.val_df.text[dataset.val_df.category==cat].tolist()[:5]\n",
    "    return samples\n",
    "\n",
    "val_samples = get_samples() # first 5 titles of each category from validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#title = input(\"Enter a news title to classify: \")\n",
    "classifier = classifier.to(\"cpu\")\n",
    "\n",
    "for truth, sample_group in val_samples.items():\n",
    "    print(f\"True Category: {truth}\")\n",
    "    print(\"=\"*30)\n",
    "    for sample in sample_group:\n",
    "        prediction = predict_category(sample, classifier, \n",
    "                                      vectorizer, dataset._max_seq_length)\n",
    "        print(\"Prediction: {} (p={:0.2f})\".format(prediction['category'],\n",
    "                                                  prediction['probability']))\n",
    "        print(\"\\t + Sample: {}\".format(sample))\n",
    "    print(\"-\"*30 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise:\n",
    "\n",
    "1. Change F.max_pool1d() to F.avg_pool1d().\n",
    "2. Change use_glove=True to use_glove=False.\n",
    "3. Change other hyperparameters.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "138px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "threshold": "5",
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
